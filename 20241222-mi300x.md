### MI300X vs H100 vs H200 벤치마크 1부: 학습 – 여전히 강력한 CUDA 생태계

##### 요점정리: 학습 성능, 사용자 경험, 사용성, Nvidia, AMD, GEMM, 어텐션(Attention), 네트워킹, 인피니밴드(InfiniBand), 스펙트럼-X(Spectrum-X) 이더넷, RoCEv2 이더넷, SHARP, 총 소유 비용


### 1. 소개 ###

SemiAnalysis는 MI300X의 실제 성능을 검증하기 위해 5개월간의 조사를 진행해왔다. 이론적으로 MI300X는 사양 및 총 소유 비용(TCO) 측면에서 Nvidia의 H100 및 H200에 비해 큰 장점을 가져야 한다. 그러나 아래에 나열된 사양은 실제 환경에서 기대할 수 있는 성능을 제대로 나타나지 않는다. 만약 AMD가 이 메모리를 통해 광고된 성능을 제공할 수 있다면, 시장에서 매우 강력한 경쟁자가 될 수 있을 것이다. 

[Source: SemiAnalysis, Nvidia, AMD]

오늘 우리는 MI300X, H100, H200에 대한 독립적인 분석과 학습 중심 벤치마킹을 진행한 5개월간의 여정을 이야기하려고 한다. 이 과정에서 우리는 NVIDIA와 AMD 양쪽과 소통하며 작업을 진행한다. 우리가 실행한 다양한 저수준 벤치마크에 대한 상세한 개요를 제공할 예정이며, 요약은 목차에서 확인할 수 있다. 또한, NVIDIA와 AMD GPU의 총 소유 비용(TCO)을 비교하고 성능을 포함한 여러 요소를 고려할 것이다. 우리는 궁극적으로 AMD가 경쟁력을 갖추기 위해 해야 할 일과, 5개월 동안 버그를 제출하고 수정하며 확인한 AMD 소프트웨어 문제를 어떻게 해결해야 할지에 대한 종합적인 공공 권고안을 공개적으로 제공할 것이다. 문제는 단순히 소프트웨어가 미숙하다는 점만이 아니라, AMD의 소프트웨어 개발 방식 자체를 바꿔야 한다는 것이다. 

요약하자면, NVIDIA GPU와 AMD의 MI300X를 비교했을 때, MI300X가 이론적으로는 우위를 가질 가능성이 있었지만, AMD의 공개 소프트웨어 스택의 부족과 테스트 부재로 인해 그 잠재력이 실현되지 못했다. AMD의 소프트웨어 경험은 다수의 버그로 가득 차 있어, AMD를 사용한 기본(out of the box) 학습은 사실상 불가능한 상태이다. 우리는 AMD가 학습 워크로드에서 NVIDIA에 대항할 강력한 경쟁자로 떠오를 수 있기를 기대했지만, 오늘날 기준으로는 유감스럽게도 그렇지 못했다. AMD는 CUDA 생태계의 격차(CUDA moat)를 넘지 못하고 있으며, 이는 AMD의 예상보다 약한 소프트웨어 품질 관리(QA) 문화와 미흡한 초기 사용 경험(out of the box experience) 때문이다.**AMD가 CUDA 격차를 메우기 위해 노력하는 동안, NVIDIA 엔지니어들은 새로운 기능, 라이브러리, 성능 업데이트를 통해 그 격차를 더욱 깊게 만드는 데 주야로 작업하고 있다.**

SemiAnalysis는 GEMM 벤치마크 및 단일 노드 학습에 대한 소스 코드와 중간 테스트 결과를 Nvidia와 AMD 양측에 공유했으며, 피드백을 수집하고 벤치마크를 개선하기 위해 여러 차례 회의와 논의를 진행했다. 또한 AMD와 협력해 소프트웨어 스택의 버그를 수정했다. 이러한 반복적인 상호작용의 목표는, 실제 사용자들이 경험할 법한 결과를 편견 없이 평가하는 테스트를 보장하는 것이었다.

SemiAnalysis는 이 기사를 몇 달 전 공개하려 했으나, AMD 팀과의 추가 협업과 잠재적인 수정 사항을 탐구하기 위해 더 많은 시간을 할애하기로 했다. AMD 소프트웨어 스택 버그로 인해 MI300X의 성능이 제한적으로 보이지 않도록, 가능한 모든 기회를 제공하기 위해 AMD의 소프트웨어 버그를 확인하고 수정하는 데 상당한 시간을 투자했다. 이를 통해 문제만 드러나는 초기 성능 대신, 개선된 성능을 보여줄 수 있도록 했다. 또한, 이 과정에서 얼마나 많은 튜닝 작업과 버그 수정을 거쳤는지도 설명하여 공정한 인상을 줄 수 있도록 했다. 이러한 접근 방식은 사용자들에게 가능한 최고의 투명성을 제공한다고 생각한다.

**SemiAnalysis는 AMD 생태계를 개선하기 위해 기여할 수 있는 모든 방법을 모색했다. SemiAnalysis의 버그 리포트와 테스트 덕분에 AMD 소프트웨어는 상당히 개선되었지만, 여전히 공개 소프트웨어 스택은 부족한 점이 많다.** SemiAnalysis는 많은 벤치마크를 오픈소스로 공개했으며, 이를 재현할 수 있는 한 줄(one-liner) 명령어도 만들어 배포했다.

리사 수와 AMD 리더십이 소프트웨어와 테스트 스택에 대한 투자를 두 배로 늘리고 초점을 맞춘다면, Nvidia와 학습 분야에서 경쟁할 기회를 가질 수 있다. SemiAnalysis는 AMD 엔지니어들이 매우 유능하며, AMD 생태계를 발전시키기 위해 최선을 다하고 있다고 생각한다. 실제로, 이 엔지니어들의 지원(버그 수정, 설정 도움, 커스텀 이미지 제공 등)은 SemiAnalysis가 MI300X에서 더 나은 결과를 얻는 데 큰 도움이 되었다.

벤치마크 프로세스를 마무리하기 위해, 2024년 11월 15일, SemiAnalysis는 Nvidia와 AMD에 GEMM 및 단일 노드 벤치마크 코드와 주요 결과 초안을 전달하며, 의견, 검증 및 튜닝을 요청했다. 최종 의견, 수정, 피드백, 성능 개선 요청 마감일은 11월 25일로 설정했다. 이 시간표는 심층 분석과 논평 작성, 내부 및 외부 리뷰를 여러 차례 진행할 시간을 확보하기 위해 마련된 것으로, 일반적으로 이러한 과정은 2~4주 정도 소요된다.

며칠 전, 기사 12월 20일 공개 예정일을 양측에 알린 후, AMD는 개발자 브랜치의 베타 WIP 개발 빌드를 기반으로 한 결과를 포함하기 위해 공개를 연기해달라고 요청했다. 반면, Nvidia에 대한 모든 벤치마크는 공개적으로 이용 가능한 안정 빌드에서 진행되었다. 투명성과 공정성을 위해, SemiAnalysis는 11월 25일 기준 이미지와 최신 공개 소프트웨어를 기반으로 한 결과를 모두 포함했다. 하지만, 결과를 올바르게 해석하려면 AMD/Nvidia의 안정적 공개 빌드 성능에 초점을 맞추는 것이 적절하다고 본다.

**아래는 우리가 벤치마킹에 사용한 소프트웨어 빌드 목록입니다.각 빌드는 Nvidia와 AMD의 현재와 향후 성능을 비교하고, 특히 AMD의 안정 빌드와 개발 빌드가 제공할 성능 향상을 평가하기 위해 사용되었다:**

	1.	H100 Public Stable Release – Nvidia H100의 기본(out of box) 경험을 제공하는 공개 안정 빌드.
	2.	H200 Public Stable Release – Nvidia H200의 기본 경험을 제공하는 공개 안정 빌드.
	3.	MI300X Nov 25th Custom Build – AMD의 주요 엔지니어들이 제작한 VIP 도커 이미지로, 모든 종속성을 소스 코드에서 직접 빌드.
	4.	MI300X Stable Public Release PyTorch 2.5.1 – AMD MI300X의 기본 경험을 제공하는 공개 안정 빌드.
	5.	MI300X Public Nightly Dec 19th – 2025년 1월 PyTorch 2.6이 출시될 때 예상되는 AMD 성능을 보여주는 공개 나이틀리 빌드.
	6.	MI300X Dec 21st WIP Dev Build – 기사 공개 연기 후 AMD가 제출한 실험적 개발 빌드. 아직 AMD 내부 메인 브랜치에 병합되지 않았으며, PyTorch Flash Attention API를 사용하지 않음. 이 빌드 성능은 향후 1~2분기 내 AMD의 공개 안정 빌드 성능을 예측할 수 있음.

이번 과정에서 AMD와 Nvidia가 제공한 기술 지원에 깊이 감사드리며, SemiAnalysis가 공개하는 결과는 독립적으로 유지하고 있음을 밝힌다. AMD 측에서는 아뉘쉬 엘랑고반(AMD AI 부사장), 후이 류, 그리고 수십 명의 뛰어난 AMD 주요/수석 엔지니어, 엔지니어링 부사장(VP), 펠로우(Fellow), CVP, 디렉터, 소프트웨어 라이브러리 리더들이 우리의 버그 리포트를 신속히 처리하고 수정해 주신 점에 감사를 전한다. 또한, Nvidia 측에서는 케다르 포트다르, 이안 벅, 실뱅 주쥐, 그리고 NCCL 팀에게 훌륭한 지원에 대해 감사드린다. 

추가로, 컴퓨팅 리소스를 제공하고 오픈소스 벤치마킹을 지원해준 [Crusoe](https://crusoe.ai/cloud), [TensorWave(AMD Ventures Portco)](https://tensorwave.com/), [Nebius](https://nebius.com/), [Lambda](https://lambdalabs.com/), [Hot Aisle](https://hotaisle.xyz/), [Sustainable Metal Cloud(SMC)](https://smc.co/) / [Firmus](https://firmus.co/)에도 감사드린다. Crusoe, Nebius, SMC / Firmus, Lambda는 기본적으로 SLURM 관리와 공유 홈 디렉터리를 지원한다. TensorWave는 현재 베타 버전으로 SLURM 관리를 제공하고 있으며, 이 기능은 내년 초에 정식 출시될 예정이다. 특히 Sustainable Metal Cloud는 [공식적으로 MLPerf GPT-3 175B 학습 결과를](https://mlcommons.org/benchmarks/training/) 보유한 몇 안 되는 네오클라우드 중 하나이다. 앞으로 H100, H200, MI300X에 대한 추론 성능 관련 후속 기사를 공개할 예정이다. 또한, 몇 달 뒤에는 AMD 학습 성능과 초기 상태(out of box) 경험이 개선되었는지 확인하고 LlaVa, Mamba와 같은 모델을 테스트하는 후속 기사도 발행할 수 있다.

SemiAnalysis는 H100, H200, MI300X에 대한 추론 성능 관련 후속 기사를 공개할 예정이다. 또한, 몇 달 후에는 AMD 학습 성능을 점검하기 위해 후속 기사를 발행할 수 있으며, 이 과정에서 기본 설정 경험이 개선되었는지 확인하고 LlaVa, Mamba와 같은 다른 모델도 테스트할 예정이다. 

[Source: SemiAnalysis]

### 2. 주요 발견사항 ###

	•	이론적 FLOP/s 및 HBM 대역폭/용량 비교는 단순히 카메라의 화소 수를 비교하는 것과 같다. 실제 성능을 확인하려면 벤치마크를 실행해야 한다.
	•	Nvidia 기본 성능 및 경험은 훌륭하며, 벤치마크 과정에서 Nvidia 소프트웨어 버그를 발견하지 않았다. Nvidia는 기술 지원을 위해 엔지니어 한 명을 배정했지만, 특별한 문제가 없어서 많은 지원이 필요하지 않았다.
	•	AMD 기본 경험은 다루기 어려우며, 사용 가능한 상태로 만들기 위해 상당한 노력과 인내가 필요하다. 대부분의 벤치마크에서 AMD PyTorch 공개 안정 빌드는 여전히 문제가 있으며, 이를 해결하기 위해 우회 방법이 필요했다.
	•	AMD 엔지니어링 팀의 버그 수정 지원이 없었다면, AMD의 결과는 Nvidia보다 훨씬 낮았을 것이다.
	•	SemiAnalysis는 Sustainable Metal Cloud와 협력해 256 H100으로 비공식 MLPerf GPT-3 175B 학습을 실행하며 VBoost 설정의 효과를 테스트했다.
	•	AMD 공개 안정 빌드 소프트웨어에서의 실제 성능은 광고된 TFLOP/s와 거리가 멀다. Nvidia도 마찬가지로 실제 성능이 광고된 TFLOP/s에 미치지 못하지만, 차이가 AMD만큼 크지 않다.
	•	MI300X는 H100/H200 대비 총 소유 비용(TCO)이 낮지만, TCO 대비 학습 성능은 AMD 소프트웨어 안정 빌드 기준으로 더 낮다. 단, AMD 커스텀 개발 빌드를 사용하면 이 결과는 달라진다.
	•	MI300X의 학습 성능은 AMD의 행렬 곱셈 벤치마크에서 약점이 드러났으며, 단일 노드 학습 처리량은 여전히 Nvidia H100/H200에 뒤처진다.
	•	AMD 소프트웨어의 BF16 개발 브랜치는 성능이 더 좋지만, 아직 메인 브랜치에 병합되지 않았으며, PyTorch 안정 빌드에 통합되기까지 시간이 걸린다. 이때쯤이면 Nvidia Blackwell이 이미 출시될 것이다.
	•	MI300X는 AMD의 약한 ROCm Compute Communication Library(RCCL)와 네트워킹 및 스위칭 하드웨어에 대한 수직 통합 부족으로 인해 스케일 아웃 성능이 약하다. 반면 Nvidia는 NCCL, InfiniBand/Spectrum-X 네트워크 패브릭 및 스위치를 통해 강력한 통합을 보인다.
	•	AMD AI 라이브러리의 상당수는 Nvidia AI 라이브러리를 포크(fork)한 것으로, 최적의 결과를 내지 못하거나 호환성 문제를 초래한다.
	•	AMD 고객들은 주로 추론에 대해 수작업으로 설계된 커널만 사용하며, 이는 정의된 좁은 사용 사례 외 성능이 낮고, 급변하는 워크로드에 유연성이 거의 없다.

 ### 3. AMD에 대한 경영진 권고사항 ###

SemiAnalysis는 Nvidia와 경쟁할 수 있는 또 다른 효과적인 경쟁자가 등장하길 진심으로 바라며, AMD가 그 위치에 도달할 수 있도록 돕고 싶다. 그러나 현재로서는 AMD가 해결해야 할 과제가 많이 남아 있다. 아래는 리사 수와 AMD 경영진에 대한 상세 피드백을 요약한 내용이다.
 
	* 엔지니어 리소스 강화:AMD 엔지니어들에게 더 많은 컴퓨팅 및 엔지니어링 리소스를 제공해야 한다. 현재 AMD 엔지니어들은 Nvidia 엔지니어들이 사용하는 내부 GPU 박스 수의 일부만 보유하고 있다. AMD GPU 클라우드의 최대 제공자인 Tensorwave가 AMD 팀에 소프트웨어 문제를 해결하도록 무료 GPU 시간을 제공했다는 점은 매우 비정상적이다.
 
	* PyTorch CI/CD에 GPU 연결: MI300X와 MI325X 수천 대를 PyTorch CI/CD에 연결해 자동 테스트를 진행하고 AMD 성능 저하와 기능적 버그를 방지해야 한다. Nvidia는 수천 대의 GPU를 PyTorch CI/CD에 제공해 훌륭한 기본 경험을 보장하고 있다.
 
	* 내부 테스트 강화:AMD 경영진은 내부 빌드 테스트에만 집중하지 말고, 공공에 출시될 제품을 직접 테스트(일명 “dogfooding”)해야 한다. 가능하면 이를 라이브 스트리밍(Twitch.tv)으로 진행해 진정한 기본 경험을 보여주는 것이 좋다. 이는 geohotz가 라이브로 작업을 스트리밍하는 방식과 유사하다.
 
	* Meta와 협력 강화:AMD는 PyTorch ROCm에서 대규모 LLM 학습 워크로드를 조속히 작동시키기 위해 Meta와 협력해야 한다. Meta에서 사용하지 않는 PyTorch 코드 경로는 종종 버그가 많다.
 
	* 환경 플래그 최소화: 수십 개의 환경 플래그를 적절히 설정하지 않으면 AMD 배포를 사용할 수 없는 상황을 개선해야 한다. 대신 이러한 설정을 **기본 구성(default configuration)**에 포함시켜 기본 사용 경험을 개선하라.
 
	* VIP 이미지만 의존하지 말 것: 모든 종속성을 소스 코드에서 빌드(main@specificcommit)하는 커스텀 VIP 이미지에 지나치게 의존하지 말고, 기본 설정 경험이 훌륭하도록 집중해야 한다. VIP 이미지는 빌드에만 5시간이 소요된다.
 
	* PYTORCH_TUNABLE_OPS 폐지: PYTORCH_TUNABLE_OPS는 프로토타입 버그 기능으로, 사용자 시간을 낭비한다. 사용자가 코드를 변경할 때마다 튜닝에 약 1시간이 소요된다. 이는 사용자에게 불편을 초래한다.
 
	* MLPerf GPT-3 175B 제출: AMD는 MLPerf 학습 GPT-3 175B 결과를 제출해야 한다. MLPerf는 시간 대비 수렴도를 기준으로 비교하는 객관적인 벤치마크 방식이다.
 
	* 피드백 수용: AMD 데이터센터 GPU 생태계를 개선하기 위해 필요한 방법에 대해 더욱 구체적인 피드백으로 논의할 준비가 되어 있다.

SemiAnalysis는 AMD가 경쟁력을 갖추길 바라며, AMD가 더 나은 생태계를 구축할 수 있도록 계속 협력할 것이다.

 ### 4. AMD vs Nvidia 논쟁 요약 ###

AMD의 소프트웨어 스택의 여러 측면이 AMD의 발전을 방해하는 이유를 논하기 전에, MI300X의 기본 사양, 총 소유 비용(TCO) 비교, 그리고 대부분의 분석가와 투자자가 평가한 경쟁력에 대해 살펴보겠습니다. MI300X는 2023년 말 출시되며 흥미로운 이론적 사양을 선보였다.

	•	FP16 연산: 1,307 TFLOP/s (H100의 989 TFLOP/s보다 강력).
	•	메모리 대역폭: 5.3 TB/s.
	•	HBM3 용량: 192GB.

이 사양은 H200보다 우위에 있으며, H200은 H100의 메모리 스펙 업그레이드 버전으로, 4.8 TB/s 메모리 대역폭과 141GB HBM3e를 제공한다. 이처럼 MI300X는 H100과 H200의 사양을 능가하며 높은 경쟁력을 갖춘 것으로 평가되었다.

[Source: SemiAnalysis, Nvidia, AMD]

MI300X의 총 소유 비용은 매우 매력적이다. 이는 MI300X의 낮은 ASP(평균 판매 가격(ASP, Average Selling Price) 뿐만 아니라, 보통 더 저렴한 이더넷 네트워킹으로 배포되기 때문이다. 16k H200 클러스터와 16k MI300X 이더넷 클러스터를 비교하면 네트워킹 비용에서만 약 40% 절감되며, 나머지는 더 낮은 가속기 비용에서 나온다. 특히, 화이트박스 이더넷 스위치는 Nvidia Quantum-2 스위치를 사용하는 것보다 훨씬 저렴하다. 하지만 가장 큰 차이는 트랜시버 가격이다. Nvidia 브랜드 트랜시버는 일반 OEM 트랜시버보다 2~3배 비싸기 때문이다.

표면적으로 MI300X는 높은 성능과 낮은 TCO를 동시에 제공하는 최적의 솔루션처럼 보인다. 출시 당시, 이런 조합은 AMD가 시장 점유율을 늘릴 논리적인 기반이 될 것으로 보였다. 아래 표는 클러스터 초기 CAPEX(자본 지출(CAPEX, Capital Expenditure)를 보여준다. 클러스터 구성 요소와 네트워킹 자체 명세서(BoM, Bill of Materials)에 대한 더 자세한 분석은 기사 하단 섹션에서 확인할 수 있다.

[Source: SemiAnalysis AI TCO Model](https://semianalysis.com/ai-cloud-tco-model/)

주문이 확정되면서 MI300X에 대한 기대감이 커졌고, AMD의 긍정적인 전망과 가이던스가 이를 더욱 부추겼다. 매력적인 사양 우위를 고려할 때, AMD의 가이던스가 보수적이라는 전제하에 추가 성장 가능성을 기대하기는 쉬운 일이었다. 이론적으로 AMD는 강력한 위치에 있었다. 2024년 데이터센터 GPU 시장 점유율이 한 자릿수 중반대였고, 2027년까지 10~12% 점유율에 도달하는 것이 보수적인 예측으로 보이면서 AMD에 상당한 수익 성장 가능성을 제공할 것으로 기대됐다.

그러나 2023년 말부터 2024년 대부분의 기간 동안, 2024년 전체 데이터센터 GPU 매출에 대한 가이던스는 이러한 높은 기대치를 계속해서 밑돌았다. AMD는 1분기 실적 발표에서 3분기 실적 발표까지 가이던스를 40억 달러에서 50억 달러로 상향 조정했지만, 이는 CoWoS 및 HBM 공급 계약을 기반으로 투자자들이 예상했던 60~80억 달러에 크게 못 미쳤다. SemiAnalysis의 Accelerator Model에서는 마이크로소프트의 연초 실망과 후속 주문 부재가 초기부터 수요 전망에 부정적인 영향을 미쳤다.

초기의 긍정적인 논리는 마치 자동차 잡지에서 특정 모델을 보고 시승이나 실제 소유자들의 피드백, 리뷰 없이 구매 결정을 내리는 것과 비슷했다. 하지만 걱정하지 않아도 된다. SemiAnalysis는 MI300X, H100, H200을 대규모 벤치마크를 통해 철저히 검증했으며, 현재의 AMD 소프트웨어 스택 문제들이 이 논리를 결정적으로 반박하는 이유를 보여줄 수 있다.

### 5. 일반 행렬 곱셈 성능 ###

대부분의 FLOPS는 챗GPT나 라마와 같은 트랜스포머 기반 아키텍처에서 행렬 곱셈(GEMM. General Matrix Multiply) 작업에 사용된다. 이 때문에, GEMM 성능은 ChatGPT, Llama, Claude, Grok 등과 같은 최신 트랜스포머 모델이 특정 하드웨어에서 얼마나 잘 학습할 수 있는지를 나타내는 지표로 적합하다.

GEMM은 두 개의 입력 행렬, 즉 행렬 A와 행렬 B를 사용한다. 행렬 A는 (M, K) 크기(M개의 행, K개의 열)를 가지며, 행렬 B는 (K, N) 크기를 가진다. 이를 곱하면 결과로 (M, N) 크기의 출력 행렬이 생성된다.

[Source: Nvidia]

개념적으로, 결과 행렬의 각 요소는 입력 행렬의 “K” 차원을 따라 이루어진 요소별 곱(element-wise multiplication)의 합이다. 이러한 이유로 K 차원은 축소 차원(reduction dimension)이라고도 불린다.

[Source: SemiAnalysis]

아래는 실제 환경에서 사용된 다양한 행렬 크기(M, N, K)를 테스트한 결과다. 이는 (M, K) 크기의 행렬과 (K, N) 크기의 행렬을 곱하는 과정을 나타낸다.

다음 행렬 크기는 [Meta의 Llama 70B 모델](https://github.com/pytorch-labs/float8_experimental/blob/fe6e08c867abf56b1acd0f34473c69cde624f0a3/benchmarks/bench_matmul.py#L57) 학습 과정에서 실제로 사용되었다:
	•	(16384, 8192, 1280) – QKV 프로젝션 GEMM 형태
	•	(16384, 1024, 8192) – 어텐션 출력 프로젝션 형태
	•	(16384, 8192, 7168) – FFN GEMM 형태
	•	(16384, 3584, 8192) – FFN GEMM 형태
	•	(8192, 8192, 8192) – 벤치마크용 표준 GEMM 형태

SemiAnalysis는 OpenAI의 do_bench 함수를 사용해 벤치마크를 설정했다. 이 함수는 PyTorch 벤치마크에서 업계 표준으로 사용되며, 실행 간 캐시 초기화를 기본으로 제공하고, 여러 번 웜업 및 반복 실행한 뒤 중간값을 결과로 산출한다. 이번 테스트에서는 warmup=30과 rep=200 설정을 사용했다.

입력 텐서 A와 B는 각각 평균 0, 분산 1의 정규 분포를 사용해 무작위로 초기화했다. 이는 정규 분포가 현대 신경망에서 가중치와 활성화 값의 실제 분포와 가장 유사하기 때문이다. 입력 텐서의 분포는 TFLOP/s 성능 벤치마크 결과에 영향을 미칠 수 있으며, 이에 대한 이유는 기사 후반부에서 다룬다.

BF16 기준으로, H100과 H200은 광고된 989.5 TFLOP/s에 비해 약 720 TFLOP/s를 기록한 반면, MI300X는 광고된 1,307 TFLOP/s에 크게 못 미치는 약 620 TFLOP/s를 기록했다. 이는 MI300X가 광고된 BF16 TFLOP/s 성능이 훨씬 높음에도 불구하고, H100 및 H200보다 약 14% 느리다는 것을 의미한다. 이 AMD 결과는 AMD 주요 엔지니어가 직접 제작한 커스텀 도커 이미지를 사용한 테스트 결과임에도 Nvidia GPU보다 성능이 낮았다. 더욱이, 기본(out of box) 상태로 MI300X를 테스트했을 때 TFLOP/s 처리량은 이보다도 더 낮았다.

AMD는 이러한 성능을 얻기 위해 커스텀 이미지만 사용하는 것이 아니라, 기본값으로 설정되지 않은 수많은 환경 플래그를 사용자가 직접 설정해야 했다.

[Source: SemiAnalysis]

안타깝게도 FP8 성능은 더 심각하다. H100/H200은 광고된 1,979 TFLOP/s 중 약 1,280 TFLOP/s를 기록했다. 반면, MI300X는 약 990 TFLOP/s에 그쳤다. 따라서 FP8 기준으로 MI300X는 H100보다 22% 느리다. 이는 두 입력이 모두 e4m3 FP8(지수 비트 4개, 가수 비트 3개) 데이터 타입일 때의 결과다.

[Source: SemiAnalysis]

GEMM 호출은 단순한 작업이기 때문에 AMD 소프트웨어에서 버그가 발생하지 않을 것으로 기대할 수 있다. 하지만 이번 여름 몇 달 동안 AMD에서 발견된 주요 버그 중 하나는 torch.matmul과 F.Linear API가 서로 다른 성능을 보였다는 점이다. 일반적으로 torch.matmul과 F.Linear는 동일한 성능을 보여야 하지만, 놀랍게도 F.Linear가 훨씬 느렸다.

이 버그는 이상한 문제다. torch.matmul과 F.Linear는 모두 하드웨어 벤더의 GEMM 라이브러리를 래핑한 형태이기 때문에 동일한 성능을 내야 한다. 특히 F.Linear는 PyTorch에서 대부분의 사용자가 GEMM 커널을 실행하는 주요 방식이기 때문에 중요하다.

우리가 5개월 전 AMD 테스트를 시작했을 때, AMD의 공개 PyTorch 버전은 여전히 이 버그를 가지고 있었다. 이 버그의 근본 원인은 AMD가 실제로 두 가지 서로 다른 GEMM 라이브러리를 사용하고 있다는 점에 있었다. 즉, rocBLAS와 hipBLASLt인데, 이 중 hipBLASLt가 MI300X에 더 최적화되어 있다. 문제는 torch.matmul은 최적화된 hipBLASLt를 사용하는 반면, AMD는 F.Linear를 기본값으로 변경하지 않아 여전히 최적화되지 않은 rocBLAS 라이브러리를 사용하게 놔둔 것이었다.

이 심각한 버그는 우리가 버그 리포트를 제출한 후 몇 달 전에 AMD가 수정했다. 하지만 적절한 회귀 테스트가 부족한 탓에 이 문제가 다시 발생하지 않기를 바란다. AMD는 사용자들이 이러한 치명적인 문제를 발견하기 전에 테스트를 강화하면 사용 편의성이 크게 개선될 수 있다.

우리는 테스트에 사용한 GEMM 벤치마크를 간단한 3줄 코드로 오픈소스화했으며, 누구나 쉽게 실행할 수 있다.

[Source: SemiAnalysis]

### 5. 일반적인 GEMM 벤치마크는 정확하지 않다. ###

최근 인터넷에서 AMD MI300X의 GEMM 성능이 H100과 거의 비슷하다는 주장이 담긴 벤치마크가 확산되고 있다.

[Source: Github]

이 벤치마크에는 두 가지 주요 문제가 있다. L2 캐시 초기화를 제대로 수행하지 않고 있으며, 특정 행렬 크기에 대한 반복 실행 동안 최대 성능 값만 가져오고, 중간값(median) 또는 평균값(mean) TFLOP/s를 사용하지 않는다. 반복 실행 사이에 L2 캐시를 초기화하지 않으면, 이 벤치마크는 실제 환경에서의 GEMM 성능을 정확히 반영하지 못한다. 또한 TFLOP/s는 반복 단계에 따라 달라지기 때문에, 정확한 GEMM 벤치마크를 위해서는 100번 이상의 반복에서 평균값이나 중간값을 기준으로 사용해야 한다. OpenAI의 do_bench는 기본적으로 L2 캐시 초기화와 평균/중간값 계산을 제공하므로, 마이크로 벤치마크에 이를 사용하는 것을 권장한다. 아래는 벤치마크를 간단히 의사 코드(pseudocode)로 정리한 내용과 위에서 언급한 문제들에 대한 주석이다.

[Source: SemiAnalysis]

### 6. HBM 메모리 대역폭 성능 ###

AMD MI300X가 Nvidia H100 및 H200보다 더 나은 메모리 대역폭을 제공한다는 것은 널리 알려져 있다. MI300X는 5.3 TB/s의 대역폭을 제공하며, 이는 H200의 4.8 TB/s와 H100의 3.35 TB/s보다 뛰어나다. 향상된 HBM 메모리 대역폭은 추론 작업에서 매우 유용하며, 때로는 학습에서도 도움이 된다.

학습에서는 HBM 메모리 용량과 대역폭이 더 크면 **배치 크기(batch size)**를 더 크게 설정할 수 있다. 하지만 글로벌 배치 크기가 너무 커지면 모델의 **수렴 시간(time to convergence)**이 길어질 수 있다. 글로벌 배치 크기를 키우면 빠르게 실행할 수는 있지만, 최종적으로는 수렴 시간에 부정적인 영향을 줄 수 있다.

우리가 진행한 HBM 메모리 대역폭 벤치마크 결과, MI300X는 H200과 H100보다 확실히 더 뛰어난 메모리 대역폭을 보여주었다. 우리는 PyTorch에서 **Tensor.copy_**를 사용해 메모리 대역폭을 테스트했으며, 정확성을 보장하기 위해 업계 표준인 OpenAI의 do_bench를 활용했다. 곧 공개될 H100 vs H200 vs MI300X 추론 기사에서 볼 수 있듯이, 메모리 대역폭은 추론 작업에서 매우 중요한 요소다.

[Source: SemiAnalysis]

[Source: SemiAnalysis]

### 7. AMD의 수작업 VIP 커스텀 빌드와 WIP 개발 빌드 ###

AMD 성능을 H100/H200 성능의 75% 수준까지 끌어올릴 수 있었던 유일한 이유는 AMD의 여러 팀이 수많은 AMD 소프트웨어 버그를 수정하는 데 지원을 제공했기 때문이다. AMD를 사용 가능한 상태로 만들고 어느 정도 합리적인 성능을 확보하기 위해, AMD 주요 엔지니어가 직접 제작한 약 60개의 명령어로 구성된 Dockerfile을 제공받았다. 이 Dockerfile은 종속성과 하위 종속성(hipBLASLt, Triton, PyTorch, TransformerEngine)을 소스에서 빌드하며, 이를 실행하는 데만 약 5시간이 걸린다. 반면, Nvidia는 사전 빌드된(out of box) 환경을 제공하며 단 한 줄의 코드로 설치가 가능하다는 점에서 큰 차이가 있다. 대부분의 사용자는 PyTorch나 hipBLASLt를 소스에서 빌드하지 않고 안정된 릴리스 버전을 사용한다.

공개된 PyTorch를 사용할 때, 사용자는 최신 안정 이미지나 야간 빌드(nightly) 중에서 선택할 수 있다. 야간 빌드는 최신 커밋을 포함하고 있어 성능 향상이나 일부 버그 수정을 기대할 수 있지만, 이러한 빌드는 충분히 테스트되지 않았을 수 있으며 Meta/AMD/Nvidia 또는 다른 PyTorch 기여자가 발견하지 못한 새로운 버그가 포함될 가능성도 있다. 대부분의 최종 사용자는 PyTorch 안정 릴리스를 사용하는 점을 주목해야 한다.

[Source: SemiAnalysis, AMD]

[Source: Nvidia]

기쁘게도 Nvidia의 Docker 이미지는 Nsight Compute, Nsight Systems와 같은 프로파일링 및 디버깅에 필요한 개발자 도구를 완전히 포함하고 있다. 반면, AMD는 OmniTrace 개발자 도구를 기본으로 제공하지 않는다. 몇 주 전까지만 해도 AMD의 Docker 이미지는 PyTorch 2.3만 지원했으며, 이 버전은 8개월 전에 출시된 것이다. 하지만 이후로 PyTorch 2.4, PyTorch 2.5가 출시되었고, PyTorch 2.6은 2025년 1분기에 출시될 예정이다. 우리는 AMD 주요 엔지니어와 AMD AI 부사장에게 AMD PyTorch 최신 버전을 제공해야 한다고 권장했다. 이후 AMD는 일부 PyTorch 버전에 대한 컨테이너를 제공하기 시작했다. 하지만 여전히 PyTorch 2.5에 대한 Docker 이미지는 제공되지 않고 있다.

[Source: Nvidia]

### 8. 12월 21일 AMD 개발 빌드 ###

아래는 AMD의 12월 21일 개발 빌드 도커 이미지다. 이 이미지에서는 hipBLASLt, AOTriton, ROCm Attention과 같은 종속성의 안정되지 않은 개발 브랜치를 사용하며, PyTorch를 포함한 모든 종속성을 소스 코드에서 빌드한다. 이 빌드에는 5시간 이상이 소요되며, 이러한 종속성들은 아직 AMD의 메인 브랜치에도 병합되지 않았다. **사용자의 99.9%는 PyTorch와 그 모든 종속성을 개발 브랜치 소스 코드에서 빌드하지 않으며, 대신 공개된 안정 PyPi PyTorch를 사용할 것이다.**

게다가, PyTorch에서 기본으로 제공하는 Flash Attention API인 [torch.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) 대신, 이 AMD 개발 빌드는 또 다른 라이브러리(개발 브랜치 기반)의 어텐션 구현을 가져와 사용한다. 하지만 [torch.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) API는 사용자 친화적이고 PyTorch에 기본 포함되어 있어 더 많은 사용자가 이 방식을 선호한다. [심지어 AMD의 공개 문서에서도 torch.scaled_dot_product_attention API를 통해 Flash Attention을 사용하는 것을 권장한다.](https://rocm.blogs.amd.com/artificial-intelligence/flash-attention/README.html#benchmarking-attention) 우리는 이러한 커널들이 PyTorch의 Flash Attention에 병합되길 바란다. 사용자들에게 별도의 라이브러리를 설치하도록 요구하고, 이를 빌드하는 데 몇 시간을 소모하게 하는 것은 결코 사용자 친화적이지 않다. 또한 AMD는 FlexAttention도 지원해야 한다. 이는 빠르게 업계의 표준이 되어가고 있기 때문이다.

AMD의 12월 21일 개발 빌드는 완전한 품질 보증(QA)를 거치지 않은 위험 브랜치(risk branch)에서 진행된 빌드다. 이는 실제 사용자 환경에서는 거의 사용되지 않는 방식으로, 소스 코드에서 빌드하거나 개발 브랜치 기반으로 실행한 결과의 신뢰성에 대해 많은 우려가 있다. 대부분의 사용자는 PyPI 안정 릴리스에서 AMD/Nvidia PyTorch를 설치하기 때문에, 이러한 결과를 분석할 때 이 점을 염두에 두길 권장한다.

그럼에도 불구하고, 우리는 이 개발 빌드 결과를 포함시켰다. 이는 AMD의 공개 안정 릴리스 소프트웨어가 12분기 후에 어떤 상태에 도달할지 보여주는 지표가 되기 때문이다. 하지만 동시에, 12분기 후에는 Nvidia의 Blackwell이 이미 널리 배포되어 있을 예정이고, AMD의 MI355X는 2025년 하반기에나 출하를 시작할 것이다.

[Source: SemiAnalysis, AMD]

### 9. 훈련 테스트 방법론 (GPT1.5B, Llama 8B, Llama 70B, Mistral) ###

훈련 성능을 테스트하는 방법은 여러 가지가 있다. 가장 정확한 방법은 중간 규모의 AI 스타트업 모델의 내부 코드베이스를 사용해 512~1024 GPU 클러스터에서 실행해 보는 것이다. 이렇게 하면 일반 사용자가 사용하는 최적화가 모두 적용된 상태에서 테스트를 진행할 수 있다. 다른 모든 방법은 이러한 훈련 성능을 간접적으로 평가하는 방법일 뿐이다. 훈련 성능은 HBM 대역폭, HBM 용량, TFLOP/s, 네트워킹, 시스템 아키텍처를 종합적으로 고려한다. **HBM 대역폭과 용량을 이론상으로 비교하는 것은 마치 카메라의 메가픽셀 수치만으로 비교하는 것과 비슷하다.**

MLPerf GPT3 175B Training도 특정 수렴(convergence)에 도달하는 데 걸리는 시간을 측정하기 위한 좋은 기준이 될 수 있다. MLPerf 벤치마크는 글로벌 배치 크기와 혼합 정밀도(mixed precision) 구현이 수렴 속도에 미치는 영향을 고려한다. 하지만 MLPerf는 사용자 친화적인 문서와 지침이 부족해 실행하기 어렵고, 성능이 주로 MLPerf를 위해 특수하게 조정된 구성으로 최적화되기 때문에 일반 사용자가 쉽게 채택하기 어렵다. 참고로, Nvidia는 11k H100 이상의 GPU로 MLPerf Training 결과를 제출한 반면, AMD는 내부적으로만 MLPerf Training을 실행한다. AMD의 결과는 성능이 약할 가능성이 높아 MLPerf Training 결과는 물론, MLPerf GPT3 175B 벤치마크조차 제출하지 않았다.

우리가 SemiAnalysis 벤치마크를 설계할 때, 일반 사용자의 모델 구현을 반영하고자 했으며, 이를 위해 flash attention 백엔드를 사용하는 API인 [torch.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html), PyTorch 의 분산 데이터 병렬(DDP, Distributed Data Parallel) 및/또는 **완전 샤딩 데이터 병렬 처리(FSDP, Fully Sharded Data Parallel)**와 [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)을 선택했다. [참고로 AMD도 자체 문서에서 torch.scaled_dot_product_attention 사용을 권장하고 있다.](https://rocm.blogs.amd.com/artificial-intelligence/flash-attention/README.html#benchmarking-attention) 우리는 이것이 일반적인 사용자 워크로드를 가장 잘 대표한다고 판단했다.

또한, 우리는 이 모델들을 PyTorch 네이티브 구현 방식으로 사용하여, 일반적인 ML 연구자들이 사용하는 방식과 유사하게 만들었고, 단 한 줄의 코드로 쉽게 실행할 수 있도록 설계했다. 이는 복잡한 MLPerf와 달리, 실행을 가능한 간단하게 하면서도 성능을 적절히 반영하는 것을 목표로 한다. 다만, 이 벤치마크는 수렴 시간(time to convergence)을 고려하지 않기 때문에, AMD 쪽에 약간 유리하게 작동한다. 이는 AMD에서 Nvidia보다 마이크로 배치 크기를 더 높게 설정했기 때문이다. 수렴 시간을 고려하면 AMD의 결과는 현재 보고된 것보다 더 나빠질 것이다.

여담으로, 많은 AI 실무자들은 Megatron, NeMo, **3D 병렬 처리(Parallelism)**를 사용하지 않는다고 말한다. 이는 이 라이브러리들이 너무 복잡하고 유연성이 부족해 ML 연구에 효과적으로 활용하기 어렵기 때문이다. 3D 병렬 처리의 경우, AMD와 Nvidia 모두 소프트웨어 스택이 제대로 작동한다는 전제하에 더 높은 성능을 얻을 수 있다. 하지만 AMD의 경우, 이 전제가 크게 흔들릴 가능성이 있다. AMD Megatron은 Nvidia Megatron의 **포크(fork)**이며, 10개 미만의 스타를 가지고 있어 제대로 **내부 테스트(dogfooding)**되지 않았을 가능성이 높다. 간단한 모델에서 AMD Megatron을 작동시키기 위해 버그 리포트를 제출하고 문제를 해결하는 데 **몇 달이** 더 걸릴 수도 있다.

SemiAnalysis 모델 학습 벤치마크에서는 4개의 모델을 테스트했다. 첫 번째는 GPT 1.5B DDP로, 이는 대규모 모델로 확장하기 전에 소규모 실험이나 소거법 실험이 어떻게 보일지 잘 보여준다고 판단했기 때문이다. DDP는 훨씬 단순하고 네트워크 부담이 적은 병렬 처리 방식이다. 다음으로, 인기 있는 모델의 성능 기준선으로 Llama3 8B와 Llama3 70B 4 Layer Proxy를 테스트했다. 세 번째로는 Mistral 7B v0.1을 테스트했는데, 이는 하드웨어가 약간의 복잡성을 추가했을 때 얼마나 잘 작동하는지 평가하기 위함이다. Mistral은 표준 Causal Attention 대신 슬라이딩 윈도우 어텐션을 사용한다. ChatGPT, Claude, Gemini, o1, o3와 같은 최신 모델들은 더 이상 표준 Causal Attention을 사용하지 않고 복잡한 어텐션 메커니즘을 채택하고 있다.

현대적인 GPT/Llama/Transformer 모델은 동일한 Transformer Layer를 반복적으로 쌓아 올려 구성된다. 따라서 단 4개의 레이어 성능을 측정하는 것만으로도 모델 전체 성능을 예측하는 데 좋은 기준이 된다.

[Source: Imgur]

또한, 최신 대규모 언어 모델(LLM) 학습에서는 모든 최첨단 LLM 모델에 **파이프라인 병렬 처리(pipeline parallelism)**가 사용된다. 이는 몇 개의 트랜스포머 계층들이 각 GPU 서버에 분산 배치된다는 것을 의미한다. 최신 사전 학습(pretraining)에서는 전체 모델이 단일 노드에 배치되는 일이 절대 없다.

[Source: SemiAnalysis]

각 토큰을 학습하는 데 필요한 모델 FLOP은 아래 공식으로 정의한다:

6 * non_input_embedding_params + 12 * num_layers * num_heads * head_dim * max_seq_len * density

여기서 density는 전체 마스크 대비 어텐션의 희소도를 나타낸다. 예를 들어, Causal Attention은 희소도가 50%이며, 슬라이딩 윈도우 어텐션은 이보다 더 낮은 희소도를 가진다. 참고로, 초기에는 6 * params를 사용해 모델 FLOP를 계산했는데, 이는 잘못된 방식이었다. 올바른 계산법은 6 * non_input_embedding_params를 사용하는 것이다. 또한, 우리가 FSDP를 사용하는 방식에도 버그가 있었다.

이후, 테스트 도구를 업데이트하고 모든 벤치마크 결과를 소급해서 다시 테스트했으며, H100, H200, MI300X에 대한 공개 안정 버전, 공개 야간 빌드, VIP 이미지, AMD 개발 빌드 모두에 대해 결과를 업데이트했다. 아래에 나오는 모든 결과는 업데이트된 테스트 도구를 사용해 산출한 것이다.

### 10. 단일 노드 훈련 성능 ###

이번 보고서에서 제시한 H100/H200의 성능은 Nvidia 엔지니어가 수작업으로 조정한 튜닝 없이 제공되는 기본 상태(out of the box) 성능을 반영한 것이다. 반면, MI300X의 결과는 AMD 엔지니어들이 몇 달간 튜닝하고 버그를 수정한 후 얻은 것이다. Nvidia에서는 특정한 버그를 만나지 않았던 반면, AMD의 학습 환경은 상대적으로 많은 버그로 가득 차 있었다. 5개월 전만 해도, AMD MI300X에서 많은 모델이 150 TFLOP/s 이상의 성능을 내지 못했는데, 이는 **어텐션 역전파(attention backwards)**와 torch.compile에서 발생한 AMD 소프트웨어 버그 때문이었다. 이 버그로 인해 사용자는 모델의 일부 영역을 **비컴파일(non-compliable)**로 수동으로 설정해야 했으며, 전체 그래프를 컴파일할 수 없었다.

모든 모델에서 H100/H200이 MI300X의 공개 릴리스, 공개 야간 빌드, 11월 25일 VIP 빌드 대비 성능이 더 우수하다는 것을 확인할 수 있었다. 특히 GPT 1.5B와 같은 소형 모델이나 비-Causal 어텐션 레이어를 사용하는 Mistral 7B v0.1 같은 모델에서 MI300X의 성능이 좋지 않다는 점이 흥미롭다. 이는 FlexAttention이 마감 시점까지 완전히 작동하지 않았기 때문이며, Nvidia GPU에서는 2024년 8월부터 이미 안정적으로 작동하고 있었다. 그 결과, MI300X 공개 릴리스 버전, 공개 야간 빌드, 11월 25일 VIP 빌드 기준으로 H100/H200이 TFLOP/s 성능에서 MI300X보다 2.5배 이상 앞서는 것을 확인할 수 있었다.

12월 21일 MI300X 내부 WIP 개발 브랜치 빌드에서도 GPT 1.5B에서는 여전히 H100/H200보다 성능이 낮았다. 또한, Mistral 7B에서는 H100보다 약간 낮은 성능을 보였다. 하지만 Llama3 8B와 Llama3 70B Proxy에서는 12월 21일 MI300X WIP 개발 빌드가 H100/H200보다 더 나은 성능을 보였다. 다만, 이는 MI300X WIP 개발 버전이 AMD 엔지니어의 개발 브랜치를 사용한 결과로, 이 브랜치는 아직 AMD 메인 브랜치에 병합되지 않은 상태라는 점을 유의해야 한다.

[Source: SemiAnalysis]

3개월 전만 해도 AMD에서 FP8 훈련을 시도하면 **세그멘테이션 오류(segfaults)**와 심각한 에러가 발생했다. 간혹 작동하더라도, 실제로는 BF16을 사용할 때보다 더 느렸다. 우리는 AMD의 FP8 팀과 협력해 이 문제를 해결했으며, AMD hipBLASLt 팀과 함께 MI300X의 FP8 성능을 개선하기 위한 튜닝 작업도 진행했다. FP8 훈련은 BF16보다 훈련 속도를 높여주기 때문에 중요하며, 대부분의 최첨단 연구소들이 FP8 훈련을 사용하고 있다.

여러 문제를 수정한 후, MI300X의 11월 25일 빌드에서는 Llama3 8B와 GPT 1.5B에서의 처리량이 어느 정도 H100과 경쟁할 만한 수준으로 개선되었다. 하지만, H200은 여전히 이 부문에서 승리했다. 그러나, Llama3 70B 4 Layer Proxy에서는 AMD의 11월 25일 결과가 크게 뒤처졌다.

특히, **비-인과적 어텐션 레이어(non-causal attention layer)**를 사용하는 Mistral 7B에서는 AMD 11월 25일 성능이 H100의 절반 수준에 불과했다. 이는 모델 구조에 약간의 변형만 있어도, 몇 달간의 튜닝을 거친 후에도 AMD가 여전히 경쟁력이 없음을 보여준다. 많은 최첨단 모델과 AI 훈련 스타트업들은 긴 문맥 처리와 효율적인 어텐션을 위해 복잡한 어텐션 레이어를 사용하고 있지만, AMD는 여전히 이 부분에서 많이 뒤처져 있다.

안타깝게도, AMD의 FP8 훈련은 11월 25일 VIP 이미지나 12월 21일 WIP 개발 브랜치 이미지 같은 커스텀 이미지에서만 작동한다. 처음 AMD FP8 훈련을 시도했을 때는 공개 릴리스에서는 FP8 훈련이 AMD BF16 훈련보다 더 느렸다.

[Source: SemiAnalysis]

AMD의 WIP 개발 빌드에서는 Llama3 8B에서 H100보다 나은 성능을 보였지만, 여전히 H200의 공개 안정 버전 소프트웨어보다는 느렸다. H200의 성능은 12월 21일 AMD WIP 개발 브랜치에서도 MI300X를 완전히 압도했다. 흥미로운 점은, Mistral 7B v0.1과 같은 **비-인과적 어텐션 레이어(non-causal attention layer)**를 사용하는 모델에서 MI300X가 내부 빌드에서도 성능이 좋지 않다는 것이다. Mistral은 **슬라이딩 윈도우 어텐션(sliding window attention)**을 사용하며, 이는 일부 최첨단 모델에서도 사용된다. 비-인과적 어텐션을 사용하지 않는 모델을 학습하려 한다면, AMD MI300X는 자동으로 경쟁에서 밀릴 가능성이 크다.

하드웨어 간 성능 비교를 제공하는 사람들이 많지만, 대부분은 테스트 코드를 오픈소스로 공개하지 않으며, 쉽게 재현할 수 있는 방식으로 만들지 않는다. 우리는 오픈소스 방식을 채택했으며, 단일 노드 훈련 벤치마크를 공개했다. 또한, 단 몇 줄의 코드만으로 실행할 수 있도록 간단하게 만들었다:

[Source: SemiAnalysis]

### 10. 멀티 노드 훈련 성능 ###

멀티 노드 테스트에서는 H100 두 노드와 MI300X 두 노드를 벤치마크했다. 안타깝게도, 기사 작성 시점에 H200 멀티 노드 배포 환경을 사용할 수는 없었다. 이 벤치마크에서도 H100이 MI300X에 비해 큰 격차로 앞섰으며, H100은 10~25% 더 빠른 성능을 보였다. 노드를 더 추가하여 단일 훈련 작업에 투입할수록 이 격차는 더 커졌다. 이는 잘 알려진 문제로, AMD는 내년에 자사의 새로운 400G AI 특화 NIC를 배포하여 이 문제를 해결하려 하고 있다.

### 11. AMD PYTORCH_TUNABLE_OPS 플래그는 나쁜 사용자 경험 ###

AMD에서 훈련을 제대로 실행하려면 PYTORCH_TUNABLE_OPS라는 플래그를 사용해야 한다. 이 플래그는 AMD 전용 프로토타입 기능으로, 사용자가 GEMM을 튜닝할 수 있도록 만들어졌다. 하지만 이 기능은 아직 안정적이지 않은 프로토타입이기 때문에 과거에 세그멘테이션 오류(seg faults), HBM 메모리 누수, 그리고 단위 테스트 비활성화와 같은 여러 문제가 발생했다. 현재 알려진 튜닝 가능한 연산 관련 버그들은 수정되었지만, 여전히 많은 미확인 AMD 소프트웨어 버그가 존재할 가능성이 높다.

게다가, 사용자가 아무런 버그를 겪지 않는다 하더라도, 이 AMD 플래그를 제대로 사용하려면 최신 LLM 모델을 튜닝하는 데만 1~2시간이 걸린다. GEMM 튜닝 결과를 캐싱할 수는 있지만, 사용자의 코드에 사소한 변경 사항이 생기면 다시 1~2시간을 들여 튜닝을 해야 한다. 이는 모델 R&D나 실험을 반복적으로 진행해야 하는 ML 연구자의 작업 속도를 크게 저하시킬 수 있다.

반면에, Nvidia에서는 이 플래그가 필요 없다. Nvidia의 GEMM 라이브러리인 cuBLASLt는 out of the box 상태에서 이미 튜닝되어 있으며, H100/H200에서 대부분의 형태에 대해 적절한 알고리즘을 자동으로 선택한다. 하지만 AMD의 hipBLASLt/rocBLAS의 휴리스틱 모델은 기본적으로 대부분의 형태에 대해 잘못된 알고리즘을 선택하기 때문에 사용자가 시간을 들여 직접 튜닝해야 한다. AMD는 GEMM 라이브러리의 휴리스틱 모델을 개선하여 기본 상태에서 적절한 알고리즘을 선택하도록 수정할 필요가 있다. 사용자는 연구 과정에서 빠르게 반복 작업을 수행해야 하기 때문에, 튜닝 작업을 반복적으로 실행해야 한다면 연구 속도가 크게 저하될 것이다.

### 12. NVLink/xGMI 토폴로지 확장 ###

GPU 클러스터에서는 **스케일업 패브릭(scale up fabric)**이 매우 중요하다. 이는 최첨단 모델 학습에 사용되는 텐서 병렬 처리와 익스퍼트(expert) 병렬 처리에 매우 빠른 경로를 제공하기 때문이다. 이러한 이유로, 우리는 스케일업 패브릭 성능을 측정하기 위한 벤치마크를 진행했다.

H100과 H200에서의 스케일업 패브릭은 NVLink라 불리며, GPU당 450GB/s의 대역폭을 제공하며 8개의 GPU를 연결한다. 반면, MI300X의 스케일업 패브릭은 xGMI라 불리며, 사양상 8개의 GPU를 연결하고 GPU당 448GB/s의 대역폭을 제공한다. 표면적으로 보면, MI300X의 스케일업 네트워크는 H100/H200과 매우 유사하며, 이론상으로는 대역폭이 0.5% 낮을 뿐이다. 하지만 실제로는 상황이 크게 다르다.

우선, MI300X의 xGMI는 포인트 투 포인트(point-to-point) 방식의 패브릭이다. 이는 실제로 GPU 쌍 간에 448GB/s의 대역폭을 제공하지 않는다는 뜻이다. 대신, 각 GPU는 서로에게 64GB/s의 대역폭만으로 통신할 수 있다. 하나의 GPU가 나머지 7개의 GPU 모두와 동시에 통신해야만 448GB/s의 대역폭에 도달할 수 있다. 따라서, **텐서 병렬 처리(TP)**에서 TP=2인 경우 최대 대역폭은 64GB/s, TP=4인 경우는 189GB/s에 불과하다.

[Source: SemiAnalysis]

반면, Nvidia의 NVLink는 **스위치형 토폴로지(switched topography)**를 사용하기 때문에, 한 GPU가 다른 GPU와 통신할 때 450GB/s의 전체 대역폭을 사용할 수 있다. 또한, H100/H200에 있는 4개의 NVSwitch는 **네트워크 내 감소(in-network reduction)**를 지원하며, 이는 기본적으로 활성화된 NVLink SHARP (NVLS) 기술을 통해 이루어진다. 이 기술은 스위치 내부에서 집계/감소 연산을 수행함으로써 데이터 이동을 줄이는 데 도움을 준다.

[Source: SemiAnalysis]

### 13. All Reduce/All to All/Reduce Scatter/All Gather Collectives 개요 ###

우리는 Nvidia H100/H200과 AMD MI300의 스케일업 및 스케일아웃 네트워크에 대한 벤치마크 결과를 소개할 예정이다. 테스트할 집합 연산(collectives)은 최첨단 LLM 학습에서 주로 사용되는 연산들로, all_reduce, all_gather, reduce_scatter, all to all을 포함한다.

	•	All reduce는 데이터 병렬 처리와 텐서 병렬 처리에 사용된다.
	•	All gather는 ZeRO/FSDP 병렬 처리와 텐서 병렬 처리에 사용된다.
	•	Reduce scatter는 ZeRO/FSDP 병렬 처리에 사용된다.

계산-통신(compute-communication) 중첩 방식 때문에 실제 메시지 크기는 16MiB에서 256MiB 사이가 되며, PyTorch DDP의 기본 메시지 크기는 25MiB이다. (참고로, NVIDIA MLPerf에서 11,000개의 H100으로 GPT-3 175B를 학습했을 때는 [메시지 크기가 최대 200MiB였다.](https://github.com/mlcommons/training_results_v4.1/blob/b87b9e396f771345d4ef122ba33456304f15228d/NVIDIA/benchmarks/gpt3/implementations/eos-dfw_n1452_ngc24.04_nemo/config_common.sh#L69))

또한, 최대 버스 대역폭을 확인하기 위해 8GiB와 16GiB 메시지 크기도 테스트했지만, 이러한 크기는 실제 환경에서는 사용되지 않는다. 위에서 언급된 모든 집합 연산은 3D 병렬 처리와 FSDP/ZeRO 병렬 처리 동안 사용되며, 이는 최첨단 모델 학습에 흔히 사용하는 기법들이다.

[Source: DeepSpeed]

[Source: Meta]

### 14. 단일 노드 집합 연산 ###

Nvidia는 모든 집합 연산에서 실제 메시지 크기 기준으로 AMD보다 훨씬 뛰어난 성능을 보인다. 이는 놀랍지 않은 결과다. H100/H200은 450GB/s NVLink 스위치형 토폴로지와 네트워크 내 감소(NVLS)를 지원하는 반면, MI300X는 7x64GB/s xGMI 포인트 투 포인트 토폴로지를 사용하기 때문이다.

[Source: SemiAnalysis]

[Source: SemiAnalysis]

[Source: SemiAnalysis]

[Source: SemiAnalysis]

이 테스트를 재현하려면, 우리가 개발한 오픈소스 ClusterMax-NCCL/RCCL 벤치마크를 사용할 수 있다. 이 벤치마크는 단 한 줄의 Bash 명령어로 쉽게 실행할 수 있도록 설계되었다. ClusterMax는 곧 출시될 평가 도구로, H100/B200/GB200/MI300X 네오클라우드 클러스터를 성능과 사용자 경험 측면에서 정량적/정성적으로 평가하는 데 사용된다. 곧 공개될 “ClusterMax Neocloud Evaluation | How to rent GPUs” 글을 기대해도 좋다.

[Source: SemiAnalysis]

### 15. 멀티 노드 RCCL/NCCL 집합 연산 및 스케일아웃 네트워크 벤치마크 ###

Nvidia의 H100/H200과 MI300X 모두에서 각 GPU는 **400G 네트워크 인터페이스 카드(NIC)**를 통해 다른 노드와 스케일아웃 네트워크로 연결된다. 각 GPU마다 NIC가 직접 연결된다. H100/H200의 레퍼런스 디자인은 일반적으로 InfiniBand NDR용 ConnectX-7 NIC 또는 AI 워크로드에 특화된 Nvidia의 커스텀 이더넷 솔루션인 Spectrum-X 이더넷용 BlueField-3를 사용한다. 반면, MI300X의 레퍼런스 디자인은 Broadcom Thor-2 NIC를 사용하는 RoCEv2 이더넷을 권장한다.

[Source: Nvidia]

일반적인 GPU 클러스터는 거의 항상 단일 계층 네트워크보다 더 많은 계층이 필요하다. 단일 계층 네트워크는 Broadcom 이더넷이나 Nvidia Spectrum X 이더넷의 경우 최대 128개의 GPU를, H100/H200 InfiniBand의 경우 최대 64개의 GPU만 지원할 수 있기 때문이다. 이런 멀티 계층 네트워크에서는 보통 8-레일 최적화 Fat Tree가 사용된다. 여기서 각 GPU는 개별 스위치에 연결되며, 이러한 연결을 “레일(rail)“이라고 부른다. [우리가 작성한 AI 네오클라우드 가이드 및 구조(Anatomy) 기사에서는 레일 최적화 네트워크가 어떻게 작동하는지에 대해 자세히 설명했다.] (https://semianalysis.com/2024/10/03/ai-neocloud-playbook-and-anatomy/#cluster-level-networking-bill-of-materials)

	* 참고: [AI 네오클라우드 플레이북 및 아나토미](https://semianalysis.com/2024/10/03/ai-neocloud-playbook-and-anatomy/#cluster-level-networking-bill-of-materials)

[Source: SemiAnalysis]

Nvidia의 NVLink가 스케일업 네트워크에서 NVLS를 제공하듯, Nvidia의 H100/H200 InfiniBand 스케일아웃 네트워크도 InfiniBand SHARP In-network Reduction을 제공한다. 이는 Nvidia에만 독점적으로 제공되는 기능이다. AMD는 MI300X용으로 이에 상응하는 제품을 보유하지 못하고 있다. InfiniBand SHARP는 NVLink SHARP In-network Reduction과 유사하게 작동하며, 네트워크를 통과하는 트래픽을 줄이는 방법을 제공한다. InfiniBand SHARP의 경우, 감소 연산은 Quantum-2 InfiniBand 스위치 내부에서 수행된다.

그러나 NVLink SHARP와 달리 InfiniBand SHARP는 기본적으로 UFM/IB 서브넷 매니저에서 활성화되지 않는다. 우리는 여러 네오클라우드, H100 클러스터 운영자, 그리고 AI 프런티어 연구소들과 이야기를 나눴는데, 대부분 NCCL_TIMEOUT 증가 및 네트워크 설치와 구성의 어려움 때문에 SHARP를 활성화하지 않았다고 밝혔다. Nvidia에 어떤 AI 고객들이 InfiniBand SHARP를 사용하는지 물었지만, 구체적인 답변은 받지 못했다. 만약 InfiniBand SHARP가 AI 프로덕션 워크로드에서 유용하다면, Nvidia의 마케팅 팀이 이를 성공적으로 배포했다고 대대적으로 홍보했을 가능성이 크다. 현재 InfiniBand SHARP의 제한적인 도입을 고려하면, 우리는 SHARP 활성화 여부에 따른 Nvidia의 집합 연산 성능을 비교하여 보여준다.

일부 벤치마크에서는 Nvidia의 Spectrum-X Ethernet 데이터를 Nvidia 내부 클러스터 Israel-1에서 수집했다. Spectrum-X는 xAI의 200k H100/H200 클러스터에 사용되며, Spectrum-X 레퍼런스 아키텍처 버전 1.2에서 최대 100k GPU를 지원할 수 있지만, 맞춤형 디자인으로는 최대 512k GPU까지 지원 가능하다.

또한 Google Cloud(GCP)의 H100용 사내 이더넷과 AWS의 H100/H200을 테스트 중이다. AWS는 자체 이더넷인 EFAv2/EFAv3를 사용해 이를 배포하고 있다. 이러한 결과는 다가오는 “집합 연산 심층 분석(Collective Deep Dive)” 기사에서 공유할 예정이다. 이 기사에서는 다양한 유형의 집합 연산에 대한 시각화, NCCL 프로토콜(SIMPLE, LL, LL128), NCCL 알고리즘(NVLS, NVLSTREE, RING, TREE, COLNETDIRECT, COLNETCHAIN, PAT), 그리고 GCP H100 이더넷, AWS H100/H200 EFA, InfiniBand H100, Spectrum-X 등에서 집합 연산이 실행되는 방법을 설명할 것이다.

아래는 32개의 GPU로 실행한 all reduce 집합 연산 테스트 결과다. MI300X RoCEv2는 기본 InfiniBand H100과 SHARP가 활성화된 InfiniBand H100에 비해 가장 낮은 성능을 보여준다. 간단히 말해, all reduce 성능이 낮으면 스케일아웃 트레이닝 성능도 떨어지게 된다.

[Source: SemiAnalysis]

MI300X의 성능은 집합 연산에 참여하는 GPU의 수를 스케일아웃(즉, 증가)할수록 감소한다. 현대의 프런티어 트레이닝은 최소 10만 개의 GPU를 사용하는 클러스터에서 이루어진다. MI300X RoCEv2는 16MiB에서 256MiB의 실제 메시지 크기에서 InfiniBand Non-SHARP 기준 속도의 절반 수준으로 실행된다. 아래 차트에 따르면, Nvidia Spectrum-X Ethernet은 NCCL 집합 연산 라이브러리와의 수직적 통합, 그리고 우수한 혼잡 제어와 적응형 라우팅 덕분에 InfiniBand Non-SHARP의 성능에 매우 근접한 성능을 보인다.

AMD는 내년에 출시 예정인 Pollara 400G NIC를 통해 수직적 통합을 시도하고 있으며, 이 NIC는 Ultra Ethernet을 지원해 AMD가 Nvidia와 경쟁할 수 있게 해줄 가능성이 있다. 그러나 Nvidia 역시 가만히 있지 않고, 내년 말까지 800G ConnectX-8 NIC를 생산 준비할 예정이다. 이 NIC는 AMD의 Pollara NIC보다 2배 빠른 라인 속도를 제공한다.

AMD RCCL은 Nvidia NCCL의 포크 버전이다. AMD의 RCCL 팀과 AMD 내 다른 여러 팀들은 컴퓨팅 자원과 인력이 부족해 AMD 생태계를 개선하기 어려운 상황이다. 현재 AMD RCCL 팀은 연구개발을 위해 안정적으로 접근할 수 있는 MI300X가 32대 미만이다. 집합 연산의 개선은 다수의 GPU에 접근하는 것이 핵심인데, 이런 상황은 솔직히 어이없다. AMD는 소프트웨어 팀이 더 많은 GPU를 사용할 수 있도록 투자를 늘려야 한다.

이와 대조적으로, NVIDIA의 NCCL 팀은 NVIDIA의 내부 11,000대 H100 EOS 클러스터를 연구개발(R&D)에 사용할 수 있는 자원을 보유하고 있다. 또한 NVIDIA에는 집합 통신의 전문가인 실뱅 조지(Sylvain Jeaugey)를 비롯해 세계적인 수준의 집합 통신 전문가들이 다수 근무하고 있다. 안타깝게도, AMD는 비교적 낮은 보상과 자원 부족으로 인해 집합 라이브러리 인재를 유치하는 데 실패했다. 반면 NVIDIA에서는 엔지니어들이 제한 조건부 주식(RSU)의 가치 상승 덕분에 연봉 100만 달러 이상을 받는 경우도 흔하다.

이 문제를 해결하기 위해 TensorWave와 SemiAnalysis는 현재 AMD RCCL 팀과 협력하여 집합 성능을 개선하고 있다. TensorWave는 AMD RCCL 팀이 업무를 수행하는 데 필요한 자원을 확보할 수 있도록 중형 클러스터를 AMD에 후원했다. 하지만 TensorWave가 많은 GPU를 구매한 뒤, AMD가 자체 소프트웨어 문제를 해결할 수 있도록 다시 AMD에 GPU를 제공해야 하는 상황은 말도 안 되는 일이다.

또한 주목할 점은 SHARP를 사용하지 않는 네트워크에서는 GPU 수가 두 배로 늘어날수록 all reduce 집합 연산 속도가 로그 감소한다는 것이다. 반면 SHARP를 사용하면 GPU 수에 관계없이 속도/완료 시간이 일정하게 유지된다. 현재 우리는 최대 1,024대 H100에 대한 결과를 보유하고 있으며, IB SHARP all reduce가 집합 연산에 참여하는 GPU 수에 상관없이 일정한 시간에 완료된다는 점을 확인했다. 이 내용은 곧 발표할 “Collective Deep Dive” 기사에서 공개할 예정이다.

[Source: SemiAnalysis]

All gather, all to all, 그리고 reduce scatter 집합 연산에서 MI300X는 InfiniBand에 비해 2~4배 느리다. 아쉽게도, Spectrum-X나 InfiniBand SHARP의 all gather 또는 reduce scatter 벤치마크 데이터에는 접근할 수 없었다. 

[Source: SemiAnalysis]

[Source: SemiAnalysis]

[Source: SemiAnalysis]

아래는 우리가 사용한 NCCL/RCCL 벤치마크 스크립트입니다. 아쉽게도 클러스터별 설정의 특성상 단순히 한 줄로 실행할 수 있는 스크립트는 아니다. 올바르게 실행하려면 NCCL/RCCL과 NCCL-tests/RCCL-tests의 README.md를 따라야 한다. AWS나 Google Cloud를 사용하는 경우, 별도의 맞춤형 NCCL 어댑터를 설치해야 할 수도 있다.

[Source: SemiAnalysis]


