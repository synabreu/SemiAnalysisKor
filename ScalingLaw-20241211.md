### 스케일링 법칙의 명과 암 ###

##### 최근 AI 스케일링 법칙에 대해 두려움(Fear), 불확실성(Uncertainty), 의심(Doubt), 즉 FUD가 점점 커지고 있다. 파트타임 AI 산업 전망가들의 무리(“칼러베이드”)가 어떤 비관적 서사라도 붙잡고 늘어지며, 지난 몇 년간 대형 언어 모델(LLM)의 급속한 성능 향상을 이끌어온 스케일링 법칙의 종말을 선언하고 있다. 기자들 역시 이러한 집단 비난에 가세하고 있는데, 이들은 모호한 정보로 가득한 시끄러운 유출 자료들에 의존해, 모델이 예전만큼 성공적으로 스케일하지 못하는 “실패” 사례를 근거로 제시한다. 또 다른 회의론자들은 이미 포화된 벤치마크를 들며, 최신 모델들이 해당 벤치마크에서 개선의 기미를 거의 보이지 않는다는 점을 지적한다. 비평가들은 또한 이용 가능한 학습 데이터의 고갈과 학습용 하드웨어 스케일링 둔화를 문제로 삼고 있다. #####

<p align="center">
 <img src = "./scalinglaw-20241211/image01.png">
</p>

##### 이러한 불안감에도 불구하고, 대형 AI 연구소들과 하이퍼스케일러들의 데이터센터 구축 가속과 막대한 자본 지출은 그 자체로 분명한 메시지를 전한다. 예를 들어, 아마존은 Trainium2 커스텀 실리콘 개발을 가속화하고, 총 65억 달러 규모의 IT 및 데이터센터 투자를 통해 Anthropic을 위해 40만 개의 칩을 준비하고 있다. 메타는 2026년까지 루이지애나에 총 2GW급의 데이터센터를 건설할 계획이며, OpenAI와 구글 역시 단일 사이트 전력 한계를 극복하기 위해 공격적인 다중 데이터센터 훈련 계획을 내놓고 있다. 이러한 주요 의사결정자들은 스케일링 법칙이 여전히 유효하다는 확신을 조금도 흔들지 않는 듯하다. 왜 그럴까? #####

##### 출처: [Mulit-Datacenter Training: OpenAI’s Ambitious Plan to Beat Google’s Infrastructure](https://github.com/synabreu/SemiAnalysisKor/Multidatacenter-20240904.md) #####
<br>


