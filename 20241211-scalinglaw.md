## 스케일링 법칙 – O1 Pro 아키텍처, 추론 훈련 인프라, Orion 및 Claude 3.5 Opus의 ‘실패’ ##

최근 AI 스케일링 법칙에 대해 두려움(Fear), 불확실성(Uncertainty), 의심(Doubt), 즉 FUD가 점점 커지고 있다. 파트타임 AI 산업 전망가들의 무리(“칼러베이드”)가 어떤 비관적 서사라도 붙잡고 늘어지며, 지난 몇 년간 대형 언어 모델(LLM)의 급속한 성능 향상을 이끌어온 스케일링 법칙의 종말을 선언하고 있다. 기자들 역시 이러한 집단 비난에 가세하고 있는데, 이들은 모호한 정보로 가득한 시끄러운 유출 자료들에 의존해, 모델이 예전만큼 성공적으로 스케일하지 못하는 “실패” 사례를 근거로 제시한다. 또 다른 회의론자들은 이미 포화된 벤치마크를 들며, 최신 모델들이 해당 벤치마크에서 개선의 기미를 거의 보이지 않는다는 점을 지적한다. 비평가들은 또한 이용 가능한 학습 데이터의 고갈과 학습용 하드웨어 스케일링 둔화를 문제로 삼고 있다. 

[source: ]("./20241211-scalinglaw/image01.png")


이러한 불안감에도 불구하고, 대형 AI 연구소들과 하이퍼스케일러들의 데이터센터 구축 가속과 막대한 자본 지출은 그 자체로 분명한 메시지를 전한다. 예를 들어, 아마존은 Trainium2 커스텀 실리콘 개발을 가속화하고, 총 65억 달러 규모의 IT 및 데이터센터 투자를 통해 Anthropic을 위해 40만 개의 칩을 준비하고 있다. 메타는 2026년까지 루이지애나에 총 2GW급의 데이터센터를 건설할 계획이며, OpenAI와 구글 역시 단일 사이트 전력 한계를 극복하기 위해 공격적인 다중 데이터센터 훈련 계획을 내놓고 있다. 이러한 주요 의사결정자들은 스케일링 법칙이 여전히 유효하다는 확신을 조금도 흔들지 않는 듯하다. 왜 그럴까? 

##### 출처: [Mulit-Datacenter Training: OpenAI’s Ambitious Plan to Beat Google’s Infrastructure](https://github.com/synabreu/SemiAnalysisKor/20240904-multidatacenter.md) #####

### 1. 훈련 확장: 새로운 패러다임과 기존 패러다임의 지속 ###

현실적으로, 대부분의 파트타임 전망가들이 초점을 맞춰온 사전 학습(pre-training) 외에도 스케일링에는 훨씬 더 많은 차원이 존재한다. OpenAI의 o1 릴리스는 추론(reasoning) 모델의 유용성과 잠재력을 증명하며, 스케일링을 위한 새로운 미개척 영역을 열어주었다. 그러나 더 많은 연산 자원을 투입해 모델 성능을 향상시키는 기술은 이것 만이 아니다. 합성 데이터 생성(Synthetic Data Generation), PPO(Proximal Policy Optimization), 함수 검증기(Functional Verifiers), 그리고 추론을 위한 기타 학습 인프라 등 여러 분야가 추가적인 연산 투입을 통해 모델 성능을 개선할 수 있다. 이처럼 스케일링의 지형은 여전히 변하고 진화 중이며, 그에 따라 전체적인 AI 개발 프로세스 역시 가속을 거듭하고 있다. #####
부정확한 벤치마크에서 더 까다로운 벤치마크로 전환하면 진척 상황을 더욱 제대로 측정할 수 있게 될 것이다. 본 보고서에서는 기존의 사전 학습 스케일링 추세 뿐만 아니라 사후 학습(post-training) 및 추론 단계에서의 새로운 스케일링 추세도 다룰 예정이다. 여기에는 새로운 기법들이 한계를 어떻게 확장해 나가는지, 그리고 이전에 생각했던 것보다 훨씬 더 많은 학습 시간용 연산 스케일링이 왜 필요한지에 대한 내용도 포함된다. 
SemiAnalysis는 OpenAI의 o1 및 o1 Pro 아키텍처를 학습 인프라 측면과 추론 토크노믹스(tokenomics) 관점에서 살펴볼 것이다. 여기에는 비용, KV Cache 스케일링, 배치 처리(batch) 등의 주제가 포함된다. 또한 주요 AI 연구소들의 합성 데이터 및 강화 학습(RL) 인프라도 깊이 다룰 것이다. 마지막으로, Anthropic의 Claude 3.5 Opus와 OpenAI의 Orion 관련 “실패” 사례에 대한 진상을 바로잡고, 향후 스케일링 계획에 대해 논의하고자 한다.

### 2. 컴퓨팅의 가장 위대한 스케일링 법칙인 무어의 법칙에 찬사를 보냄 ###

오늘날 AI 스케일링 법칙에 대한 논쟁은 수십 년간 이어져온 컴퓨트 스케일링과 무어의 법칙을 둘러싼 논쟁과 크게 다르지 않다. CPU 연산 성능을 주로 클럭(clock) 속도라는 지표로만 측정하려는 사람들(이는 대략 데나드 스케일링이 끝나갈 무렵인 2000년대 후반 이전에 흔히 쓰이던 지표)이 있다면, 그들은 그때 이후로 전혀 진전이 없었다고 주장할 것이다. 하지만 실제로 컴퓨트 성능은 줄곧 발전해 왔다. 프로세서 클럭 속도에 한계에 부딪혔을 때, 전력 밀도와 냉각 문제에도 불구하고 초점은 멀티코어 아키텍처 및 기타 성능 향상 기법으로 옮겨갔다. 

[원본: CPU transistor densities, clock speeds, power and performance from 1970-2015 – Charles Leggett]
(./20241211-scalinglaw/image01.png)

무어의 법칙이 끝났다는 주장 역시 반도체 업계가 직면한 또 다른 벽이지만, 최근에는 이 논쟁이 한결 잠잠해졌다. 엔비디아(Nvidia)와 같은 AI 선도 기업들이 완전히 새로운 몇 가지 차원을 따라 컴퓨트 성능을 대폭 향상시켜 왔기 때문이다. 첨단 패키징 기술은 입출력(I/O) 스케일링과 함께, 레티클 크기  한계를 넘어서는 총 실리콘 면적을 활용할 수 있게 하여 컴퓨트 성능의 지속적인 발전을 가능하게 했다. 또한 칩 내부와 칩 간의 병렬 컴퓨팅, 그리고 더 넓은 고대역폭 네트워킹 도메인의 구축은 특히 추론(inference) 작업에서 칩들이 대규모로 더 효율적으로 협력할 수 있는 기반을 마련했다. 

[Source: Nvidia] (이미지 있음)

2004년의 컴퓨터 애호가들과 마찬가지로, 주류 분석가들과 기자들은 여전히 나무만 보고 숲을 못 보고 있다. 특정 추세가 둔화되는 것처럼 보여도, 스케일링과 확장에 적합한 새로운 패러다임들이 등장함에 따라 업계 전반은 여전히 폭발적인 속도로 전진하고 있는 것이다. ‘스케일링 법칙들’을 중첩 시키는 것도 가능하다. 즉, 사전 학습은 개선을 위한 여러 벡터 중 하나에 불과해질 것이고, 종합적인 ‘스케일링 법칙’은 지난 50여 년 동안 무어의 법칙이 그래왔듯 계속해서 스케일링을 지속할 것이다. 

##### 출처: [The Future of the Transistor](https://github.com/synabreu/SemiAnalysisKor/20230221-thefutureoftransistor.md) #####

### 3. 사전 학습 스케일링의 과제 – 데이터 장벽과 결함 허용 ###

사전 학습(pre-training)을 확대하는 것은 모델 성능에 상당한 향상을 가져왔지만, 현재 업계가 극복하려고 노력 중인 몇 가지 장애물이 있다. 그 중 하나의 명확한 장애물은 데이터 수집의 어려움이다. 인터넷 상의 데이터가 빠르게 증가하고는 있지만, 연산 자원 증가 속도와 비례하게 늘어나지는 않고 있다. 이 때문에 현재의 수조 개 파라미터를 가진 메가 모델들은 진칠라(Chinchilla) 최적성에 미치지 못하고 있다. 즉, 모델 파라미터에 비해 학습 토큰 수가 훨씬 적은 상황이다.

|참고|
|친칠라 확장 법칙은 연산 자원(Compute) 증가에 따라 데이터 양과 파라미터 수를 최적으로 늘리는 비율을 의미한다. 데이터가 충분치 않으면 모델의 일반화 성능이 저하되고, 반대로 데이터가 지나치게 많으면 과잉 학습(overtraining)이 발생하여 컴퓨트 자원이 낭비된다. 하지만 최적 비율에서 일부 벗어나는 것이 유용한 경우도 있다. 예를 들어, GPT-4o나 Llama와 같이 모델을 의도적으로 과잉 학습시키면 추론(inference) 비용을 크게 낮출 수 있으며, 이는 대규모 사용자 기반을 가진 공급자에게 더 선호될 수 있다.|

##### 출처: [100,000 H100 Clusters: Power, Network Topology, Ethernet vs InfiniBand, Reliability, Failures, Checkpointing](https://github.com/synabreu/SemiAnalysisKor/20240617-h100clusters.md) #####

2023년 1월, GPT-4가 출시되기 전 우리는 스케일링의 실질적 한계와 GPT-4가 이를 어떻게 돌파할 계획인지에 대해 글을 썼다. 그 이후로 모델들은 데이터가 모델 파라미터 수보다 훨씬 많은 ‘친칠라 최적(Chinchilla Optimal) 초과’ 상태에서, 데이터 부족으로 인해 ‘친칠라 최적 미만’ 상태로 이리저리 오가게 되었다. 과거에는 학습 및 추론 하드웨어의 개선으로 컴퓨트(연산 자원) 확보라는 장애물을 극복할 수 있었다.

##### 출처: [The AI Brick Wall – A Practical Limit for Scaling Dense Transformer Models, and How GPT 4 Will Break Past It](https://github.com/synabreu/SemiAnalysisKor/20230124-theaibrickwall.md) #####

현재 제기되는 ‘속도 저하(speed bumps)’ 관련 서사에 따르면, 유용한 데이터 소스(예: 교과서나 문서)는 이미 고갈되었고, 남은 것은 주로 품질이 낮은 텍스트 데이터 원천이다. 게다가 웹 데이터는 여전히 편협한 분포를 가지며, 모델이 지속적으로 일반화하기 위해서는 분포 밖(out-of-distribution)의 데이터가 더 많이 필요하다. 이로 인해 모델을 최적으로 스케일링하기가 어려워지면서 사전 학습(pre-training) 과정은 점점 더 까다로워지고 있다. 
또한 연구소에서 스케일링을 계속하는 와중에 데이터가 충분하지 않다면, 모델은 과잉 파라미터화(over-parametrized)되어 비효율적이 되며, 일반화 대신 막대한 양의 ‘암기(memorization)’에 의존하게 될 수 있다. 이에 따라 연구소들은 이 문제를 완화하기 위해 합성 데이터(synthetic data)의 활용을 점점 늘리고 있다. 
다만 이 문제는 주요 AI 연구소들에는 덜 적용된다. 예를 들어, 메타(Meta)는 공용 인터넷에 있는 데이터보다 약 100배 더 많은 양의 데이터를 보유하고 있으며, 만약 이를 적법하게 활용할 수만 있다면 다른 연구소들보다 덜한 문제로 스케일을 계속 확대할 수 있는 우위를 갖게 될 것이다. 유튜브(YouTube)에는 매일 72만 시간 분량의 동영상이 업로드되는데, 우리는 AI 연구소들이 아직 이 방대한 동영상 데이터에 담긴 정보를 학습하는 가능성을 이제 막 검토하기 시작했다고 본다. 이는 합성적으로 생성된 양질의 데이터를 활용할 수 있는 능력 외에도 해당 연구소들이 갖춘 잠재적 이점이며, 이에 대한 아키텍처는 이후에 논의할 예정이다. 

##### 출처: [GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE](https://github.com/synabreu/SemiAnalysisKor/20230710-gpt4architecture.md) #####

비디오에서 얻을 수 있는 수천 조(quadrillion) 단위의 대체 토큰을 학습하려면, 전체 학습 FLOPs(부동소수점 연산) 규모를 대대적으로 확장해야 하며, 이는 하드웨어 혁신과 시스템 엔지니어링을 통해 구현될 것이다. 예를 들어, 학습 FLOPs를 한 자리 수 더 올리려면 필요한 가속기 수가 단일 데이터센터를 넘어서는 만큼, 멀티 데이터센터 학습이 필수적이다. Project Rainier의 경우, 아마존이 Anthropic에 40만 개의 Tranium 2 칩을 제공하지만, 순수 FLOPs 관점에서 이는 10만 개 미만의 GB200에 해당하는 수준이다. Anthropic은 이러한 클러스터에서 학습을 구현하기 위해 상당한 엔지니어링 혁신을 달성해야 할 것이다. 광활한 캠퍼스나 복수의 캠퍼스에 가속기를 분산 배치하는 것 자체가 아마달의 법칙(Amdahl’s law) 으로 인해 상당한 도전을 야기하지만, 이에 대응하기 위한 해결책들이 이미 여럿 제안된 상태다. 
파라미터 스케일링과 관련한 또 다른 제약은 추론 경제성(inference economics)이다. AI 연구소들은 대규모 모델을 학습시키기 위해 막대한 투자를 자본화 할 수 있고, 이를 대규모이자 지속적으로 성장하는 사용자 기반에 서비스하거나, 내부 용도로 활용해 추가 모델 개선을 수행함으로써 그 비용을 분산시킬 수 있다. 그러나 추론 단계에 이르면, 너무 비싸거나 경제성이 없는 모델을 시장에 내놓지 않도록 주의해야 한다. 
평가(evals) 역시 포괄적이지 않다. 기존 평가로는 모델의 모든 능력이나 특성을 충분히 다루지 못한다. 예를 들어, 전이 학습(transfer learning), 즉 한 분야를 학습하면서 다른 분야에도 개선 효과가 나타나는 경우나, 문맥 내 학습(in-context learning) 분야는 아직 평가가 충분히 개발되지 않았다. 또한 예측하기 어려우나 최종 사용자에게 엄청난 가치를 제공할 수 있는 최종 사용 사례들도 항상 존재한다. 측정되는 것은 개선된다.

### 3. 더 새롭고 어려운 평가 기준에 도전 ###

새로운 평가 지표들은 모델들을 보다 세밀하게 구분하고, 직접적으로 유용한 특정 응용 분야에 초점을 맞추고 있다. 오늘날 가장 중요한 평가 중 하나인 SWE-Bench는 오픈소스 파이썬 저장소의 GitHub 이슈를 사람이 검토한 문제를 모델이 해결하도록 하는 것을 목표로 한다. 최근 클로드(Claude) 3.5 Sonnet 모델은 SWE-Bench Verified 기준 49%라는 최첨단(State of the Art) 성능을 달성했지만, 대부분의 모델들은 훨씬 낮은 수준에 머무르고 있다.
또 다른 예로는 AI 연구개발(R&D) 능력을 조사하는 벤치마크가 있는데, 일부 사람들은 이를 “가장 주목해야 할 능력”이라고 평가한다. 리서치 엔지니어링 벤치마크(RE, Research Engineering Benchmark)는 7개의 도전적이고 개방형인 ML 연구 환경으로 구성되어 있다. 인간은 일반적으로 더 긴 시간 범위에서 평가를 잘 수행하지만, 2시간이라는 짧은 시간 범위에서 최고 수준의 AI 에이전트들은 인간보다 4배 높은 점수를 달성했다. 현재 인간이 우위를 점하고 있는 위와 같은 중요한 과제들은 추론 시점 연산 능력(inference time compute)을 스케일링 하는 데에 딱 맞는 시험 무대다. 우리는 이러한 형태의 스케일링을 더 잘 활용하는 모델들이 미래에는 인간을 능가할 것으로 예상한다. 

이미지. Source: RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts![image](https://github.com/user-attachments/assets/60a8d580-624d-4afa-a414-b7d806235424)

또 다른 추세는 평가에 극도로 어렵고 전문가 수준의 문제를 포함하는 것이다. 대표적인 예로는 대학원 수준의 구글 검색 무력화 Q&A 벤치마크(Graduate-Level Google-Proof Q&A Benchmark, 이하 GPQA)와 Frontier Math가 있다. GPQA는 화학, 생물학, 물리학 분야에 걸쳐 총 448개의 객관식 문항으로 구성된다. 참고로 OpenAI는 전문성 있는 인간(예: 박사학위 소지자들)이 GPQA 다이아몬드(Diamond) 난이도에서 약 70%를 득점하는 반면, o1은 같은 문제 세트에서 78%를 기록했다고 밝혔다. 지난해, GPT-4에 검색 기능(및 CoT 기반의 기권 처리)을 적용한 경우 GPQA 다이아몬드 난이도에서 39%를 기록했다. 
또한 극도로 어려운 문제를 활용하는 또 다른 예는 FrontierMath(FM)이다. FM은 수백 개의 독창적인 수학 문제로 이루어져 있으며, 사람이 문제를 해결하는 데 몇 시간에서 며칠까지 걸릴 수 있다. 여기에는 수론, 실해석학 등 폭넓은 수학 분야가 포함된다. 이 평가의 핵심은 문제를 공개하지 않아 데이터 오염 위험을 최소화한다는 점이며, 자동 검증기를 통해 채점할 수 있어 평가 과정을 단순화할 수 있다는 것이다. 

이미지. Source: FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI![image](https://github.com/user-attachments/assets/11eeb615-fca2-425a-aac2-660eb2cee45c)

이 벤치마크에서 현재 최고 성능을 보이는 모델은 2% 수준에 머물러 있지만, 연구소들은 이를 극적으로 향상시킬 수 있으리라 기대하고 있다. Anthropic은 중기적으로 FrontierMath에서 80% 달성을 내다보고 있다. 

### 4. 사후 학습: 새로운 스케일링 영역 ###

사전 학습(pre-training)은 스케일링 법칙과 관련된 논쟁에서 주로 주목받는 영역인데, 이는 이해하기 쉽기 때문이다. 그러나 사전 학습은 AI 라이프사이클 중 극히 일부에 지나지 않는다. 모델을 사전 학습한 후에도 실제로 활용 가능하도록 만들기 위해 해야 할 일이 여전히 많다. 사전 학습의 목표는 극히 제한적으로 “다음 토큰을 정확히 예측하는 것”이다. 하지만 이는 “사용자의 요구에 응답”하거나 “특정 작업을 수행”하는 것이 궁극적 목표인 대형 언어 모델(LLM) 개발의 최종 단계와는 여전히 거리가 멀다.
이후 우리는 지도 미세조정(Supervised Fine Tuning, SFT), 강화학습(RL), 합성 데이터(Synthetic Data)에 대해 개괄적으로 살펴본 뒤, OpenAI의 O1 Pro 모델이 어떻게 작동하며 만들어졌는지에 대해 알아볼 것이다.

### 5. 지도 미세 조정 ###

지도 미세조정(Supervised Fine-Tuning, SFT)은 사후 학습(post-training) 중 가장 널리 알려진 방식이다. 이 방식에서는 코드, 수학, 지시사항 준수와 같은 특정 도메인을 다루는 입력-출력 쌍으로 구성된 엄선된 데이터셋을 모델에 제시한다. 사전 학습과 달리, 미세조정 단계에서는 데이터의 양보다 데이터 품질이 훨씬 더 중요하다. 데이터 양이 적기 때문에 연산량 부담도 상대적으로 작다. GPT가 처음에 주목받게 된 비결은 Scale AI와 같은 업체들이 제공한 엄선된 인간 생성·레이블 데이터 샘플을 활용한 것이었다. 하지만 시간이 흐름에 따라, 인간이 생성한 데이터로만 확장하는 데에는 한계에 부딪히고 있다.

### 6. 사후 학습에서 합성 데이터의 필수 역할 ###

SFT에서 가장 중요한 과제는 원하는 도메인에서 충분히 크고 높은 품질의 데이터 세트를 구성하는 것이다. 이를 통해 모델은 코드, 수학, 추론과 같은 특정 분야에서 더 잘 작동하게 되며, 전이 학습(transfer learning)을 통해 다른 분야에서도 성능이 개선되는 파급 효과를 얻을 수 있다. 예를 들어, 수학과 코딩 능력이 뛰어난 모델은 일반 추론 능력도 우수하며, 중국어와 영어로 동시에 학습한 모델은 영어로만 학습한 모델보다 영어 능력이 더 뛰어나다. 합성 데이터(synthetic data)는 고품질 데이터를 통제 가능하면서도 무한히 확장 가능한 방식으로 생성할 수 있는 차원을 열어주어, 원하는 주제에 대해 모델을 미세조정 할 수 있게 한다.
합성 데이터를 광범위하게 활용하는 것은 더 나은 모델을 만들려는 유인을 제공한다. 예를 들어, OpenAI는 다른 어떤 업체보다 먼저 GPT-4를 확보했고, 이를 이용해 다른 모델 제공업체보다 더 나은 합성 데이터 세트를 생성할 수 있었다. 이후 다른 업체들도 GPT-4에 맞먹는 모델을 확보하면서 상황이 변했다. 많은 오픈소스 및 중국 연구소의 모델들이 빠르게 따라잡을 수 있었던 중요한 이유 중 하나는 이들이 GPT-4로부터 생성된 합성 데이터를 활용했기 때문이다.
기반 모델이 과제를 판단하는 능력이 좋을수록, 학습용 데이터 세트의 품질도 향상된다. 여기에도 고유한 스케일링 법칙이 작동한다. 이렇게 해서 “새Claude 3.5 Sonnet”이 탄생했다. Anthropic은 Claude 3.5 Opus를 완성했으며, 이를 통해 적절히 스케일링 되며 양호한 성능을 발휘했다. (이와 반대되는 주장은 FUD일 뿐이다.)

그런데도 Anthropic은 이를 공개하지 않았다. 대신 Claude 3.5 Opus를 활용해 합성 데이터를 생성하고 보상 모델링(reward modeling)을 수행하여 Claude 3.5 Sonnet을 크게 개선했으며, 여기에 사용자 데이터도 활용했다. 추론 비용은 급격히 변하지 않았지만 모델의 성능은 확연히 개선되었다. 굳이 경제적 관점에서 비효율적인 3.5 Opus를 공개할 이유가 없었던 것이다. 차라리 3.5 Opus로부터 추가적인 사후 학습을 거친 3.5 Sonnet을 공개하는 편이 경제적으로 더 합리적이지 않은가?
더 많은 합성 데이터가 생성되면 더 우수한 모델이 만들어진다. 더 우수한 모델은 더 높은 품질의 합성 데이터를 제공하며, 선호도를 필터링하거나 점수를 매기는 데 더 뛰어난 판단자 역할을 한다. 합성 데이터를 활용하는 과정에는 여러 작은 스케일링 법칙이 내포되어 있으며, 이러한 법칙들이 합쳐져 더 나은 모델을 더 빠르게 개발하도록 이끈다.

### 7. 합성 데이터 예제 ###

#### 7-1. 리젝션 샘플링 ####

합성 데이터가 많이 활용되는 분야 중 하나는 코드 데이터 세트를 생성하는 것이다. 이는 주로 다양한 프로그래밍 작업이나 프롬프트를 시드(seed)로 지정한 후, 모델에게 해당 작업과 관련된 질문을 생성하도록 요청하는 방식으로 이루어진다.
이후 모델에게 가능한 솔루션 세트를 생성하도록 요청한다. 생성된 솔루션 중 관련 테스트를 통과하거나 올바르게 실행될 수 있는 것들만 학습 데이터 세트에 추가되며, 이를 통해 저품질 샘플을 걸러내는 과정은 리젝션 샘플링(Rejection Sampling)이라고 불린다. 리젝션 샘플링은 합성 데이터 생성 과정에서 매우 중요한 요소로, 학습 데이터 세트가 지도 미세조정(SFT) 또는 강화 학습(RL)에 유용할 만큼 충분히 높은 품질을 갖추도록 보장한다. 그러나 이 과정에서 생성된 많은 토큰들이 폐기되므로, 합성 데이터 생성은 많은 연산 자원을 소모한다.

이와 같은 방식으로 미세조정에 활용되는 합성 데이터 세트를 구축하는 방법은 많은 대형 AI 연구소에서 채택하고 있으며, Gemini, GPT, Llama, Claude와 같은 모델의 미세조정에 사용되고 있다. 하지만 리젝션 샘플링은 보기보다 더 복잡할 수 있다. 예를 들어, Llama의 경우 초기 응답이 틀렸을 때 모델에게 답변을 수정하도록 프롬프트를 제공했는데, 두 번째 시도에서 정답을 맞출 확률이 20%에 달했다. 합성 데이터의 유용성을 보여주는 또 다른 사례로, Meta 팀은 파이썬(Python) 코드를 PHP로 변환한 후 구문 분석 및 실행을 통해 품질을 확인하고, 이를 추가적인 SFT 데이터 세트에 포함시켜 공개 PHP 코드의 부족 문제를 해결했다. 이는 합성 데이터를 활용하여 잘 대표되지 않는 영역에서도 유용한 데이터를 신뢰할 수 있고 예측 가능하게 생성할 수 있음을 효과적으로 입증한 사례다.

[원본: Meta] 

#### 7-2. 모델에 의한 판단 ####


















