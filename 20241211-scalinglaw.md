### 스케일링 법칙의 명과 암 ###

##### 최근 AI 스케일링 법칙에 대해 두려움(Fear), 불확실성(Uncertainty), 의심(Doubt), 즉 FUD가 점점 커지고 있다. 파트타임 AI 산업 전망가들의 무리(“칼러베이드”)가 어떤 비관적 서사라도 붙잡고 늘어지며, 지난 몇 년간 대형 언어 모델(LLM)의 급속한 성능 향상을 이끌어온 스케일링 법칙의 종말을 선언하고 있다. 기자들 역시 이러한 집단 비난에 가세하고 있는데, 이들은 모호한 정보로 가득한 시끄러운 유출 자료들에 의존해, 모델이 예전만큼 성공적으로 스케일하지 못하는 “실패” 사례를 근거로 제시한다. 또 다른 회의론자들은 이미 포화된 벤치마크를 들며, 최신 모델들이 해당 벤치마크에서 개선의 기미를 거의 보이지 않는다는 점을 지적한다. 비평가들은 또한 이용 가능한 학습 데이터의 고갈과 학습용 하드웨어 스케일링 둔화를 문제로 삼고 있다. #####

<p align="center">
 <img src = "./20241211-scalinglaw/image01.png">
</p>

##### 이러한 불안감에도 불구하고, 대형 AI 연구소들과 하이퍼스케일러들의 데이터센터 구축 가속과 막대한 자본 지출은 그 자체로 분명한 메시지를 전한다. 예를 들어, 아마존은 Trainium2 커스텀 실리콘 개발을 가속화하고, 총 65억 달러 규모의 IT 및 데이터센터 투자를 통해 Anthropic을 위해 40만 개의 칩을 준비하고 있다. 메타는 2026년까지 루이지애나에 총 2GW급의 데이터센터를 건설할 계획이며, OpenAI와 구글 역시 단일 사이트 전력 한계를 극복하기 위해 공격적인 다중 데이터센터 훈련 계획을 내놓고 있다. 이러한 주요 의사결정자들은 스케일링 법칙이 여전히 유효하다는 확신을 조금도 흔들지 않는 듯하다. 왜 그럴까? #####

##### 출처: [Mulit-Datacenter Training: OpenAI’s Ambitious Plan to Beat Google’s Infrastructure](https://github.com/synabreu/SemiAnalysisKor/20240904-multidatacenter.md) #####

#### <b>1. 훈련 확장: 새로운 패러다임과 기존 패러다임의 지속</b> ####

##### 현실적으로, 대부분의 파트타임 전망가들이 초점을 맞춰온 사전 학습(pre-training) 외에도 스케일링에는 훨씬 더 많은 차원이 존재한다. OpenAI의 o1 릴리스는 추론(reasoning) 모델의 유용성과 잠재력을 증명하며, 스케일링을 위한 새로운 미개척 영역을 열어주었다. 그러나 더 많은 연산 자원을 투입해 모델 성능을 향상시키는 기술은 이것 만이 아니다. 합성 데이터 생성(Synthetic Data Generation), PPO(Proximal Policy Optimization), 함수 검증기(Functional Verifiers), 그리고 추론을 위한 기타 학습 인프라 등 여러 분야가 추가적인 연산 투입을 통해 모델 성능을 개선할 수 있다. 이처럼 스케일링의 지형은 여전히 변하고 진화 중이며, 그에 따라 전체적인 AI 개발 프로세스 역시 가속을 거듭하고 있다. #####
##### 부정확한 벤치마크에서 더 까다로운 벤치마크로 전환하면 진척 상황을 더욱 제대로 측정할 수 있게 될 것이다. 본 보고서에서는 기존의 사전 학습 스케일링 추세 뿐만 아니라 사후 학습(post-training) 및 추론 단계에서의 새로운 스케일링 추세도 다룰 예정이다. 여기에는 새로운 기법들이 한계를 어떻게 확장해 나가는지, 그리고 이전에 생각했던 것보다 훨씬 더 많은 학습 시간용 연산 스케일링이 왜 필요한지에 대한 내용도 포함된다. #####
##### 우리는 OpenAI의 o1 및 o1 Pro 아키텍처를 학습 인프라 측면과 추론 토크노믹스(tokenomics) 관점에서 살펴볼 것이다. 여기에는 비용, KV Cache 스케일링, 배치 처리(batch) 등의 주제가 포함된다. 또한 주요 AI 연구소들의 합성 데이터 및 강화 학습(RL) 인프라도 깊이 다룰 것이다. 마지막으로, Anthropic의 Claude 3.5 Opus와 OpenAI의 Orion 관련 “실패” 사례에 대한 진상을 바로잡고, 향후 스케일링 계획에 대해 논의하고자 한다. #####

#### 컴퓨팅의 가장 위대한 스케일링 법칙인 무어의 법칙에 찬사를 보냄 ####

##### 오늘날 AI 스케일링 법칙에 대한 논쟁은 수십 년간 이어져온 컴퓨트 스케일링과 무어의 법칙을 둘러싼 논쟁과 크게 다르지 않다. CPU 연산 성능을 주로 클럭(clock) 속도라는 지표로만 측정하려는 사람들(이는 대략 데나드 스케일링이 끝나갈 무렵인 2000년대 후반 이전에 흔히 쓰이던 지표)이 있다면, 그들은 그때 이후로 전혀 진전이 없었다고 주장할 것이다. 하지만 실제로 컴퓨트 성능은 줄곧 발전해 왔다. 프로세서 클럭 속도에 한계에 부딪혔을 때, 전력 밀도와 냉각 문제에도 불구하고 초점은 멀티코어 아키텍처 및 기타 성능 향상 기법으로 옮겨갔다. ##### 

[원본: CPU transistor densities, clock speeds, power and performance from 1970-2015 – Charles Leggett]
(./20241211-scalinglaw/image01.png)

##### 무어의 법칙이 끝났다는 주장 역시 반도체 업계가 직면한 또 다른 벽이지만, 최근에는 이 논쟁이 한결 잠잠해졌다. 엔비디아(Nvidia)와 같은 AI 선도 기업들이 완전히 새로운 몇 가지 차원을 따라 컴퓨트 성능을 대폭 향상시켜 왔기 때문이다. 첨단 패키징 기술은 입출력(I/O) 스케일링과 함께, 레티클 크기  한계를 넘어서는 총 실리콘 면적을 활용할 수 있게 하여 컴퓨트 성능의 지속적인 발전을 가능하게 했다. 또한 칩 내부와 칩 간의 병렬 컴퓨팅, 그리고 더 넓은 고대역폭 네트워킹 도메인의 구축은 특히 추론(inference) 작업에서 칩들이 대규모로 더 효율적으로 협력할 수 있는 기반을 마련했다. #####

Source: Nvidia (이미지 있음)

##### 2004년의 컴퓨터 애호가들과 마찬가지로, 주류 분석가들과 기자들은 여전히 나무만 보고 숲을 못 보고 있다. 특정 추세가 둔화되는 것처럼 보여도, 스케일링과 확장에 적합한 새로운 패러다임들이 등장함에 따라 업계 전반은 여전히 폭발적인 속도로 전진하고 있는 것이다. ‘스케일링 법칙들’을 중첩 시키는 것도 가능하다. 즉, 사전 학습은 개선을 위한 여러 벡터 중 하나에 불과해질 것이고, 종합적인 ‘스케일링 법칙’은 지난 50여 년 동안 무어의 법칙이 그래왔듯 계속해서 스케일링을 지속할 것이다. #####

##### 출처: [The Future of the Transistor](https://github.com/synabreu/SemiAnalysisKor/20230221-thefutureoftransistor.md) #####

#### 사전 학습 스케일링의 과제 – 데이터 장벽과 결함 허용 ####

##### 사전 학습(pre-training)을 확대하는 것은 모델 성능에 상당한 향상을 가져왔지만, 현재 업계가 극복하려고 노력 중인 몇 가지 장애물이 있다. 그 중 하나의 명확한 장애물은 데이터 수집의 어려움이다. 인터넷 상의 데이터가 빠르게 증가하고는 있지만, 연산 자원 증가 속도와 비례하게 늘어나지는 않고 있다. 이 때문에 현재의 수조 개 파라미터를 가진 메가 모델들은 진칠라(Chinchilla) 최적성에 미치지 못하고 있다. 즉, 모델 파라미터에 비해 학습 토큰 수가 훨씬 적은 상황이다. #####

##### “친칠라 확장 법칙은 연산 자원(Compute) 증가에 따라 데이터 양과 파라미터 수를 최적으로 늘리는 비율을 의미한다. 데이터가 충분치 않으면 모델의 일반화 성능이 저하되고, 반대로 데이터가 지나치게 많으면 과잉 학습(overtraining)이 발생하여 컴퓨트 자원이 낭비된다. 하지만 최적 비율에서 일부 벗어나는 것이 유용한 경우도 있다. 예를 들어, GPT-4o나 Llama와 같이 모델을 의도적으로 과잉 학습시키면 추론(inference) 비용을 크게 낮출 수 있으며, 이는 대규모 사용자 기반을 가진 공급자에게 더 선호될 수 있다.” ##### 

##### 출처: [100,000 H100 Clusters: Power, Network Topology, Ethernet vs InfiniBand, Reliability, Failures, Checkpointing](https://github.com/synabreu/SemiAnalysisKor/20240617-h100clusters.md) #####

##### 2023년 1월, GPT-4가 출시되기 전 우리는 스케일링의 실질적 한계와 GPT-4가 이를 어떻게 돌파할 계획인지에 대해 글을 썼다. 그 이후로 모델들은 데이터가 모델 파라미터 수보다 훨씬 많은 ‘친칠라 최적(Chinchilla Optimal) 초과’ 상태에서, 데이터 부족으로 인해 ‘친칠라 최적 미만’ 상태로 이리저리 오가게 되었다. 과거에는 학습 및 추론 하드웨어의 개선으로 컴퓨트(연산 자원) 확보라는 장애물을 극복할 수 있었다. #####

##### 출처: [The AI Brick Wall – A Practical Limit for Scaling Dense Transformer Models, and How GPT 4 Will Break Past It](https://github.com/synabreu/SemiAnalysisKor/20230124-theaibrickwall.md) #####

##### 현재 제기되는 ‘속도 저하(speed bumps)’ 관련 서사에 따르면, 유용한 데이터 소스(예: 교과서나 문서)는 이미 고갈되었고, 남은 것은 주로 품질이 낮은 텍스트 데이터 원천이다. 게다가 웹 데이터는 여전히 편협한 분포를 가지며, 모델이 지속적으로 일반화하기 위해서는 분포 밖(out-of-distribution)의 데이터가 더 많이 필요하다. 이로 인해 모델을 최적으로 스케일링하기가 어려워지면서 사전 학습(pre-training) 과정은 점점 더 까다로워지고 있다. #####
##### 또한 연구소에서 스케일링을 계속하는 와중에 데이터가 충분하지 않다면, 모델은 과잉 파라미터화(over-parametrized)되어 비효율적이 되며, 일반화 대신 막대한 양의 ‘암기(memorization)’에 의존하게 될 수 있다. 이에 따라 연구소들은 이 문제를 완화하기 위해 합성 데이터(synthetic data)의 활용을 점점 늘리고 있다. #####
##### 다만 이 문제는 주요 AI 연구소들에는 덜 적용된다. 예를 들어, 메타(Meta)는 공용 인터넷에 있는 데이터보다 약 100배 더 많은 양의 데이터를 보유하고 있으며, 만약 이를 적법하게 활용할 수만 있다면 다른 연구소들보다 덜한 문제로 스케일을 계속 확대할 수 있는 우위를 갖게 될 것이다. 유튜브(YouTube)에는 매일 72만 시간 분량의 동영상이 업로드되는데, 우리는 AI 연구소들이 아직 이 방대한 동영상 데이터에 담긴 정보를 학습하는 가능성을 이제 막 검토하기 시작했다고 본다. 이는 합성적으로 생성된 양질의 데이터를 활용할 수 있는 능력 외에도 해당 연구소들이 갖춘 잠재적 이점이며, 이에 대한 아키텍처는 이후에 논의할 예정이다. #####

##### 출처: [GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE](https://github.com/synabreu/SemiAnalysisKor/20230710-gpt4architecture.md) #####

##### 비디오에서 얻을 수 있는 수천 조(quadrillion) 단위의 대체 토큰을 학습하려면, 전체 학습 FLOPs(부동소수점 연산) 규모를 대대적으로 확장해야 하며, 이는 하드웨어 혁신과 시스템 엔지니어링을 통해 구현될 것이다. 예를 들어, 학습 FLOPs를 한 자리 수 더 올리려면 필요한 가속기 수가 단일 데이터센터를 넘어서는 만큼, 멀티 데이터센터 학습이 필수적이다. Project Rainier의 경우, 아마존이 Anthropic에 40만 개의 Tranium 2 칩을 제공하지만, 순수 FLOPs 관점에서 이는 10만 개 미만의 GB200에 해당하는 수준이다. Anthropic은 이러한 클러스터에서 학습을 구현하기 위해 상당한 엔지니어링 혁신을 달성해야 할 것이다. 광활한 캠퍼스나 복수의 캠퍼스에 가속기를 분산 배치하는 것 자체가 아마달의 법칙(Amdahl’s law) 으로 인해 상당한 도전을 야기하지만, 이에 대응하기 위한 해결책들이 이미 여럿 제안된 상태다. #####
##### 파라미터 스케일링과 관련한 또 다른 제약은 추론 경제성(inference economics)이다. AI 연구소들은 대규모 모델을 학습시키기 위해 막대한 투자를 자본화 할 수 있고, 이를 대규모이자 지속적으로 성장하는 사용자 기반에 서비스하거나, 내부 용도로 활용해 추가 모델 개선을 수행함으로써 그 비용을 분산시킬 수 있다. 그러나 추론 단계에 이르면, 너무 비싸거나 경제성이 없는 모델을 시장에 내놓지 않도록 주의해야 한다. #####
##### 평가(evals) 역시 포괄적이지 않다. 기존 평가로는 모델의 모든 능력이나 특성을 충분히 다루지 못한다. 예를 들어, 전이 학습(transfer learning), 즉 한 분야를 학습하면서 다른 분야에도 개선 효과가 나타나는 경우나, 문맥 내 학습(in-context learning) 분야는 아직 평가가 충분히 개발되지 않았다. 또한 예측하기 어려우나 최종 사용자에게 엄청난 가치를 제공할 수 있는 최종 사용 사례들도 항상 존재한다. 측정되는 것은 개선된다. #####

#### 더 새롭고 어려운 평가 기준에 도전 ####

##### 새로운 평가 지표들은 모델들을 보다 세밀하게 구분하고, 직접적으로 유용한 특정 응용 분야에 초점을 맞추고 있다. 오늘날 가장 중요한 평가 중 하나인 SWE-Bench는 오픈소스 파이썬 저장소의 GitHub 이슈를 사람이 검토한 문제를 모델이 해결하도록 하는 것을 목표로 한다. 최근 클로드(Claude) 3.5 Sonnet 모델은 SWE-Bench Verified 기준 49%라는 최첨단(State of the Art) 성능을 달성했지만, 대부분의 모델들은 훨씬 낮은 수준에 머무르고 있다. #####
##### 또 다른 예로는 AI 연구개발(R&D) 능력을 조사하는 벤치마크가 있는데, 일부 사람들은 이를 “가장 주목해야 할 능력”이라고 평가한다. 리서치 엔지니어링 벤치마크(RE, Research Engineering Benchmark)는 7개의 도전적이고 개방형인 ML 연구 환경으로 구성되어 있다. 인간은 일반적으로 더 긴 시간 범위에서 평가를 잘 수행하지만, 2시간이라는 짧은 시간 범위에서 최고 수준의 AI 에이전트들은 인간보다 4배 높은 점수를 달성했다. 현재 인간이 우위를 점하고 있는 위와 같은 중요한 과제들은 추론 시점 연산 능력(inference time compute)을 스케일링 하는 데에 딱 맞는 시험 무대다. 우리는 이러한 형태의 스케일링을 더 잘 활용하는 모델들이 미래에는 인간을 능가할 것으로 예상한다. #####

이미지. Source: RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts![image](https://github.com/user-attachments/assets/60a8d580-624d-4afa-a414-b7d806235424)

또 다른 추세는 평가에 극도로 어렵고 전문가 수준의 문제를 포함하는 것이다. 대표적인 예로는 대학원 수준의 구글 검색 무력화 Q&A 벤치마크(Graduate-Level Google-Proof Q&A Benchmark, 이하 GPQA)와 Frontier Math가 있다. GPQA는 화학, 생물학, 물리학 분야에 걸쳐 총 448개의 객관식 문항으로 구성된다. 참고로 OpenAI는 전문성 있는 인간(예: 박사학위 소지자들)이 GPQA 다이아몬드(Diamond) 난이도에서 약 70%를 득점하는 반면, o1은 같은 문제 세트에서 78%를 기록했다고 밝혔다. 지난해, GPT-4에 검색 기능(및 CoT 기반의 기권 처리)을 적용한 경우 GPQA 다이아몬드 난이도에서 39%를 기록했다.
또한 극도로 어려운 문제를 활용하는 또 다른 예는 FrontierMath(FM)이다. FM은 수백 개의 독창적인 수학 문제로 이루어져 있으며, 사람이 문제를 해결하는 데 몇 시간에서 며칠까지 걸릴 수 있다. 여기에는 수론, 실해석학 등 폭넓은 수학 분야가 포함된다. 이 평가의 핵심은 문제를 공개하지 않아 데이터 오염 위험을 최소화한다는 점이며, 자동 검증기를 통해 채점할 수 있어 평가 과정을 단순화할 수 있다는 것이다.

이미지. Source: FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI![image](https://github.com/user-attachments/assets/11eeb615-fca2-425a-aac2-660eb2cee45c)

이 벤치마크에서 현재 최고 성능을 보이는 모델은 2% 수준에 머물러 있지만, 연구소들은 이를 극적으로 향상시킬 수 있으리라 기대하고 있다. Anthropic은 중기적으로 FrontierMath에서 80% 달성을 내다보고 있다.















