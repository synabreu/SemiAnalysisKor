## 스케일링 법칙 – O1 Pro 아키텍처, 추론 훈련 인프라, Orion 및 Claude 3.5 Opus의 ‘실패’ ##

최근 AI 스케일링 법칙에 대해 두려움(Fear), 불확실성(Uncertainty), 의심(Doubt), 즉 FUD가 점점 커지고 있다. 파트타임 AI 산업 전망가들의 무리(“칼러베이드”)가 어떤 비관적 서사라도 붙잡고 늘어지며, 지난 몇 년간 대형 언어 모델(LLM)의 급속한 성능 향상을 이끌어온 스케일링 법칙의 종말을 선언하고 있다. 기자들 역시 이러한 집단 비난에 가세하고 있는데, 이들은 모호한 정보로 가득한 시끄러운 유출 자료들에 의존해, 모델이 예전만큼 성공적으로 스케일하지 못하는 “실패” 사례를 근거로 제시한다. 또 다른 회의론자들은 이미 포화된 벤치마크를 들며, 최신 모델들이 해당 벤치마크에서 개선의 기미를 거의 보이지 않는다는 점을 지적한다. 비평가들은 또한 이용 가능한 학습 데이터의 고갈과 학습용 하드웨어 스케일링 둔화를 문제로 삼고 있다. 

![source: ]("/20241211-scalinglaw/scale-01.webp")


이러한 불안감에도 불구하고, 대형 AI 연구소들과 하이퍼스케일러들의 데이터센터 구축 가속과 막대한 자본 지출은 그 자체로 분명한 메시지를 전한다. 예를 들어, 아마존은 Trainium2 커스텀 실리콘 개발을 가속화하고, 총 65억 달러 규모의 IT 및 데이터센터 투자를 통해 Anthropic을 위해 40만 개의 칩을 준비하고 있다. 메타는 2026년까지 루이지애나에 총 2GW급의 데이터센터를 건설할 계획이며, OpenAI와 구글 역시 단일 사이트 전력 한계를 극복하기 위해 공격적인 다중 데이터센터 훈련 계획을 내놓고 있다. 이러한 주요 의사결정자들은 스케일링 법칙이 여전히 유효하다는 확신을 조금도 흔들지 않는 듯하다. 왜 그럴까? 

##### 출처: [Mulit-Datacenter Training: OpenAI’s Ambitious Plan to Beat Google’s Infrastructure](https://github.com/synabreu/SemiAnalysisKor/20240904-multidatacenter.md) #####

### 1. 훈련 확장: 새로운 패러다임과 기존 패러다임의 지속 ###

현실적으로, 대부분의 파트타임 전망가들이 초점을 맞춰온 사전 학습(pre-training) 외에도 스케일링에는 훨씬 더 많은 차원이 존재한다. OpenAI의 o1 릴리스는 추론(reasoning) 모델의 유용성과 잠재력을 증명하며, 스케일링을 위한 새로운 미개척 영역을 열어주었다. 그러나 더 많은 연산 자원을 투입해 모델 성능을 향상시키는 기술은 이것 만이 아니다. 합성 데이터 생성(Synthetic Data Generation), PPO(Proximal Policy Optimization), 함수 검증기(Functional Verifiers), 그리고 추론을 위한 기타 학습 인프라 등 여러 분야가 추가적인 연산 투입을 통해 모델 성능을 개선할 수 있다. 이처럼 스케일링의 지형은 여전히 변하고 진화 중이며, 그에 따라 전체적인 AI 개발 프로세스 역시 가속을 거듭하고 있다. #####
부정확한 벤치마크에서 더 까다로운 벤치마크로 전환하면 진척 상황을 더욱 제대로 측정할 수 있게 될 것이다. 본 보고서에서는 기존의 사전 학습 스케일링 추세 뿐만 아니라 사후 학습(post-training) 및 추론 단계에서의 새로운 스케일링 추세도 다룰 예정이다. 여기에는 새로운 기법들이 한계를 어떻게 확장해 나가는지, 그리고 이전에 생각했던 것보다 훨씬 더 많은 학습 시간용 연산 스케일링이 왜 필요한지에 대한 내용도 포함된다. 
SemiAnalysis는 OpenAI의 o1 및 o1 Pro 아키텍처를 학습 인프라 측면과 추론 토크노믹스(tokenomics) 관점에서 살펴볼 것이다. 여기에는 비용, KV Cache 스케일링, 배치 처리(batch) 등의 주제가 포함된다. 또한 주요 AI 연구소들의 합성 데이터 및 강화 학습(RL) 인프라도 깊이 다룰 것이다. 마지막으로, Anthropic의 Claude 3.5 Opus와 OpenAI의 Orion 관련 “실패” 사례에 대한 진상을 바로잡고, 향후 스케일링 계획에 대해 논의하고자 한다.

### 2. 컴퓨팅의 가장 위대한 스케일링 법칙인 무어의 법칙에 찬사를 보냄 ###

오늘날 AI 스케일링 법칙에 대한 논쟁은 수십 년간 이어져온 컴퓨트 스케일링과 무어의 법칙을 둘러싼 논쟁과 크게 다르지 않다. CPU 연산 성능을 주로 클럭(clock) 속도라는 지표로만 측정하려는 사람들(이는 대략 데나드 스케일링이 끝나갈 무렵인 2000년대 후반 이전에 흔히 쓰이던 지표)이 있다면, 그들은 그때 이후로 전혀 진전이 없었다고 주장할 것이다. 하지만 실제로 컴퓨트 성능은 줄곧 발전해 왔다. 프로세서 클럭 속도에 한계에 부딪혔을 때, 전력 밀도와 냉각 문제에도 불구하고 초점은 멀티코어 아키텍처 및 기타 성능 향상 기법으로 옮겨갔다. 

[Source: CPU transistor densities, clock speeds, power and performance from 1970-2015 – Charles Leggett]


무어의 법칙이 끝났다는 주장 역시 반도체 업계가 직면한 또 다른 벽이지만, 최근에는 이 논쟁이 한결 잠잠해졌다. 엔비디아(Nvidia)와 같은 AI 선도 기업들이 완전히 새로운 몇 가지 차원을 따라 컴퓨트 성능을 대폭 향상시켜 왔기 때문이다. 첨단 패키징 기술은 입출력(I/O) 스케일링과 함께, 레티클 크기  한계를 넘어서는 총 실리콘 면적을 활용할 수 있게 하여 컴퓨트 성능의 지속적인 발전을 가능하게 했다. 또한 칩 내부와 칩 간의 병렬 컴퓨팅, 그리고 더 넓은 고대역폭 네트워킹 도메인의 구축은 특히 추론(inference) 작업에서 칩들이 대규모로 더 효율적으로 협력할 수 있는 기반을 마련했다. 

[Source: Nvidia]

2004년의 컴퓨터 애호가들과 마찬가지로, 주류 분석가들과 기자들은 여전히 나무만 보고 숲을 못 보고 있다. 특정 추세가 둔화되는 것처럼 보여도, 스케일링과 확장에 적합한 새로운 패러다임들이 등장함에 따라 업계 전반은 여전히 폭발적인 속도로 전진하고 있는 것이다. ‘스케일링 법칙들’을 중첩 시키는 것도 가능하다. 즉, 사전 학습은 개선을 위한 여러 벡터 중 하나에 불과해질 것이고, 종합적인 ‘스케일링 법칙’은 지난 50여 년 동안 무어의 법칙이 그래왔듯 계속해서 스케일링을 지속할 것이다. 

##### 출처: [The Future of the Transistor](https://github.com/synabreu/SemiAnalysisKor/20230221-thefutureoftransistor.md) #####

### 3. 사전 학습 스케일링의 과제 – 데이터 장벽과 결함 허용 ###

사전 학습(pre-training)을 확대하는 것은 모델 성능에 상당한 향상을 가져왔지만, 현재 업계가 극복하려고 노력 중인 몇 가지 장애물이 있다. 그 중 하나의 명확한 장애물은 데이터 수집의 어려움이다. 인터넷 상의 데이터가 빠르게 증가하고는 있지만, 연산 자원 증가 속도와 비례하게 늘어나지는 않고 있다. 이 때문에 현재의 수조 개 파라미터를 가진 메가 모델들은 진칠라(Chinchilla) 최적성에 미치지 못하고 있다. 즉, 모델 파라미터에 비해 학습 토큰 수가 훨씬 적은 상황이다.

|참고|
|친칠라 확장 법칙은 연산 자원(Compute) 증가에 따라 데이터 양과 파라미터 수를 최적으로 늘리는 비율을 의미한다. 데이터가 충분치 않으면 모델의 일반화 성능이 저하되고, 반대로 데이터가 지나치게 많으면 과잉 학습(overtraining)이 발생하여 컴퓨트 자원이 낭비된다. 하지만 최적 비율에서 일부 벗어나는 것이 유용한 경우도 있다. 예를 들어, GPT-4o나 Llama와 같이 모델을 의도적으로 과잉 학습시키면 추론(inference) 비용을 크게 낮출 수 있으며, 이는 대규모 사용자 기반을 가진 공급자에게 더 선호될 수 있다.|

##### 출처: [100,000 H100 Clusters: Power, Network Topology, Ethernet vs InfiniBand, Reliability, Failures, Checkpointing](https://github.com/synabreu/SemiAnalysisKor/20240617-h100clusters.md) #####

2023년 1월, GPT-4가 출시되기 전 우리는 스케일링의 실질적 한계와 GPT-4가 이를 어떻게 돌파할 계획인지에 대해 글을 썼다. 그 이후로 모델들은 데이터가 모델 파라미터 수보다 훨씬 많은 ‘친칠라 최적(Chinchilla Optimal) 초과’ 상태에서, 데이터 부족으로 인해 ‘친칠라 최적 미만’ 상태로 이리저리 오가게 되었다. 과거에는 학습 및 추론 하드웨어의 개선으로 컴퓨트(연산 자원) 확보라는 장애물을 극복할 수 있었다.

##### 출처: [The AI Brick Wall – A Practical Limit for Scaling Dense Transformer Models, and How GPT 4 Will Break Past It](https://github.com/synabreu/SemiAnalysisKor/20230124-theaibrickwall.md) #####

현재 제기되는 ‘속도 저하(speed bumps)’ 관련 서사에 따르면, 유용한 데이터 소스(예: 교과서나 문서)는 이미 고갈되었고, 남은 것은 주로 품질이 낮은 텍스트 데이터 원천이다. 게다가 웹 데이터는 여전히 편협한 분포를 가지며, 모델이 지속적으로 일반화하기 위해서는 분포 밖(out-of-distribution)의 데이터가 더 많이 필요하다. 이로 인해 모델을 최적으로 스케일링하기가 어려워지면서 사전 학습(pre-training) 과정은 점점 더 까다로워지고 있다. 
또한 연구소에서 스케일링을 계속하는 와중에 데이터가 충분하지 않다면, 모델은 과잉 파라미터화(over-parametrized)되어 비효율적이 되며, 일반화 대신 막대한 양의 ‘암기(memorization)’에 의존하게 될 수 있다. 이에 따라 연구소들은 이 문제를 완화하기 위해 합성 데이터(synthetic data)의 활용을 점점 늘리고 있다. 
다만 이 문제는 주요 AI 연구소들에는 덜 적용된다. 예를 들어, 메타(Meta)는 공용 인터넷에 있는 데이터보다 약 100배 더 많은 양의 데이터를 보유하고 있으며, 만약 이를 적법하게 활용할 수만 있다면 다른 연구소들보다 덜한 문제로 스케일을 계속 확대할 수 있는 우위를 갖게 될 것이다. 유튜브(YouTube)에는 매일 72만 시간 분량의 동영상이 업로드되는데, 우리는 AI 연구소들이 아직 이 방대한 동영상 데이터에 담긴 정보를 학습하는 가능성을 이제 막 검토하기 시작했다고 본다. 이는 합성적으로 생성된 양질의 데이터를 활용할 수 있는 능력 외에도 해당 연구소들이 갖춘 잠재적 이점이며, 이에 대한 아키텍처는 이후에 논의할 예정이다. 

##### 출처: [GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE](https://github.com/synabreu/SemiAnalysisKor/20230710-gpt4architecture.md) #####

비디오에서 얻을 수 있는 수천 조(quadrillion) 단위의 대체 토큰을 학습하려면, 전체 학습 FLOPs(부동소수점 연산) 규모를 대대적으로 확장해야 하며, 이는 하드웨어 혁신과 시스템 엔지니어링을 통해 구현될 것이다. 예를 들어, 학습 FLOPs를 한 자리 수 더 올리려면 필요한 가속기 수가 단일 데이터센터를 넘어서는 만큼, 멀티 데이터센터 학습이 필수적이다. Project Rainier의 경우, 아마존이 Anthropic에 40만 개의 Tranium 2 칩을 제공하지만, 순수 FLOPs 관점에서 이는 10만 개 미만의 GB200에 해당하는 수준이다. Anthropic은 이러한 클러스터에서 학습을 구현하기 위해 상당한 엔지니어링 혁신을 달성해야 할 것이다. 광활한 캠퍼스나 복수의 캠퍼스에 가속기를 분산 배치하는 것 자체가 아마달의 법칙(Amdahl’s law) 으로 인해 상당한 도전을 야기하지만, 이에 대응하기 위한 해결책들이 이미 여럿 제안된 상태다. 
파라미터 스케일링과 관련한 또 다른 제약은 추론 경제성(inference economics)이다. AI 연구소들은 대규모 모델을 학습시키기 위해 막대한 투자를 자본화 할 수 있고, 이를 대규모이자 지속적으로 성장하는 사용자 기반에 서비스하거나, 내부 용도로 활용해 추가 모델 개선을 수행함으로써 그 비용을 분산시킬 수 있다. 그러나 추론 단계에 이르면, 너무 비싸거나 경제성이 없는 모델을 시장에 내놓지 않도록 주의해야 한다. 
평가(evals) 역시 포괄적이지 않다. 기존 평가로는 모델의 모든 능력이나 특성을 충분히 다루지 못한다. 예를 들어, 전이 학습(transfer learning), 즉 한 분야를 학습하면서 다른 분야에도 개선 효과가 나타나는 경우나, 문맥 내 학습(in-context learning) 분야는 아직 평가가 충분히 개발되지 않았다. 또한 예측하기 어려우나 최종 사용자에게 엄청난 가치를 제공할 수 있는 최종 사용 사례들도 항상 존재한다. 측정되는 것은 개선된다.

### 3. 더 새롭고 어려운 평가 기준에 도전 ###

새로운 평가 지표들은 모델들을 보다 세밀하게 구분하고, 직접적으로 유용한 특정 응용 분야에 초점을 맞추고 있다. 오늘날 가장 중요한 평가 중 하나인 SWE-Bench는 오픈소스 파이썬 저장소의 GitHub 이슈를 사람이 검토한 문제를 모델이 해결하도록 하는 것을 목표로 한다. 최근 클로드(Claude) 3.5 Sonnet 모델은 SWE-Bench Verified 기준 49%라는 최첨단(State of the Art) 성능을 달성했지만, 대부분의 모델들은 훨씬 낮은 수준에 머무르고 있다.
또 다른 예로는 AI 연구개발(R&D) 능력을 조사하는 벤치마크가 있는데, 일부 사람들은 이를 “가장 주목해야 할 능력”이라고 평가한다. 리서치 엔지니어링 벤치마크(RE, Research Engineering Benchmark)는 7개의 도전적이고 개방형인 ML 연구 환경으로 구성되어 있다. 인간은 일반적으로 더 긴 시간 범위에서 평가를 잘 수행하지만, 2시간이라는 짧은 시간 범위에서 최고 수준의 AI 에이전트들은 인간보다 4배 높은 점수를 달성했다. 현재 인간이 우위를 점하고 있는 위와 같은 중요한 과제들은 추론 시점 연산 능력(inference time compute)을 스케일링 하는 데에 딱 맞는 시험 무대다. 우리는 이러한 형태의 스케일링을 더 잘 활용하는 모델들이 미래에는 인간을 능가할 것으로 예상한다. 

[Source: RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts]

또 다른 추세는 평가에 극도로 어렵고 전문가 수준의 문제를 포함하는 것이다. 대표적인 예로는 대학원 수준의 구글 검색 무력화 Q&A 벤치마크(Graduate-Level Google-Proof Q&A Benchmark, 이하 GPQA)와 Frontier Math가 있다. GPQA는 화학, 생물학, 물리학 분야에 걸쳐 총 448개의 객관식 문항으로 구성된다. 참고로 OpenAI는 전문성 있는 인간(예: 박사학위 소지자들)이 GPQA 다이아몬드(Diamond) 난이도에서 약 70%를 득점하는 반면, o1은 같은 문제 세트에서 78%를 기록했다고 밝혔다. 지난해, GPT-4에 검색 기능(및 CoT 기반의 기권 처리)을 적용한 경우 GPQA 다이아몬드 난이도에서 39%를 기록했다. 
또한 극도로 어려운 문제를 활용하는 또 다른 예는 FrontierMath(FM)이다. FM은 수백 개의 독창적인 수학 문제로 이루어져 있으며, 사람이 문제를 해결하는 데 몇 시간에서 며칠까지 걸릴 수 있다. 여기에는 수론, 실해석학 등 폭넓은 수학 분야가 포함된다. 이 평가의 핵심은 문제를 공개하지 않아 데이터 오염 위험을 최소화한다는 점이며, 자동 검증기를 통해 채점할 수 있어 평가 과정을 단순화할 수 있다는 것이다. 

[Source: FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI]

이 벤치마크에서 현재 최고 성능을 보이는 모델은 2% 수준에 머물러 있지만, 연구소들은 이를 극적으로 향상시킬 수 있으리라 기대하고 있다. Anthropic은 중기적으로 FrontierMath에서 80% 달성을 내다보고 있다. 

### 4. 사후 학습: 새로운 스케일링 영역 ###

사전 학습(pre-training)은 스케일링 법칙과 관련된 논쟁에서 주로 주목받는 영역인데, 이는 이해하기 쉽기 때문이다. 그러나 사전 학습은 AI 라이프사이클 중 극히 일부에 지나지 않는다. 모델을 사전 학습한 후에도 실제로 활용 가능하도록 만들기 위해 해야 할 일이 여전히 많다. 사전 학습의 목표는 극히 제한적으로 “다음 토큰을 정확히 예측하는 것”이다. 하지만 이는 “사용자의 요구에 응답”하거나 “특정 작업을 수행”하는 것이 궁극적 목표인 대형 언어 모델(LLM) 개발의 최종 단계와는 여전히 거리가 멀다.
이후 우리는 지도 미세조정(Supervised Fine Tuning, SFT), 강화학습(RL), 합성 데이터(Synthetic Data)에 대해 개괄적으로 살펴본 뒤, OpenAI의 O1 Pro 모델이 어떻게 작동하며 만들어졌는지에 대해 알아볼 것이다.

### 5. 지도 미세 조정 ###

지도 미세조정(Supervised Fine-Tuning, SFT)은 사후 학습(post-training) 중 가장 널리 알려진 방식이다. 이 방식에서는 코드, 수학, 지시사항 준수와 같은 특정 도메인을 다루는 입력-출력 쌍으로 구성된 엄선된 데이터셋을 모델에 제시한다. 사전 학습과 달리, 미세조정 단계에서는 데이터의 양보다 데이터 품질이 훨씬 더 중요하다. 데이터 양이 적기 때문에 연산량 부담도 상대적으로 작다. GPT가 처음에 주목받게 된 비결은 Scale AI와 같은 업체들이 제공한 엄선된 인간 생성·레이블 데이터 샘플을 활용한 것이었다. 하지만 시간이 흐름에 따라, 인간이 생성한 데이터로만 확장하는 데에는 한계에 부딪히고 있다.

### 6. 사후 학습에서 합성 데이터의 필수 역할 ###

SFT에서 가장 중요한 과제는 원하는 도메인에서 충분히 크고 높은 품질의 데이터 세트를 구성하는 것이다. 이를 통해 모델은 코드, 수학, 추론과 같은 특정 분야에서 더 잘 작동하게 되며, 전이 학습(transfer learning)을 통해 다른 분야에서도 성능이 개선되는 파급 효과를 얻을 수 있다. 예를 들어, 수학과 코딩 능력이 뛰어난 모델은 일반 추론 능력도 우수하며, 중국어와 영어로 동시에 학습한 모델은 영어로만 학습한 모델보다 영어 능력이 더 뛰어나다. 합성 데이터(synthetic data)는 고품질 데이터를 통제 가능하면서도 무한히 확장 가능한 방식으로 생성할 수 있는 차원을 열어주어, 원하는 주제에 대해 모델을 미세조정 할 수 있게 한다.
합성 데이터를 광범위하게 활용하는 것은 더 나은 모델을 만들려는 유인을 제공한다. 예를 들어, OpenAI는 다른 어떤 업체보다 먼저 GPT-4를 확보했고, 이를 이용해 다른 모델 제공업체보다 더 나은 합성 데이터 세트를 생성할 수 있었다. 이후 다른 업체들도 GPT-4에 맞먹는 모델을 확보하면서 상황이 변했다. 많은 오픈소스 및 중국 연구소의 모델들이 빠르게 따라잡을 수 있었던 중요한 이유 중 하나는 이들이 GPT-4로부터 생성된 합성 데이터를 활용했기 때문이다.
기반 모델이 과제를 판단하는 능력이 좋을수록, 학습용 데이터 세트의 품질도 향상된다. 여기에도 고유한 스케일링 법칙이 작동한다. 이렇게 해서 “새Claude 3.5 Sonnet”이 탄생했다. Anthropic은 Claude 3.5 Opus를 완성했으며, 이를 통해 적절히 스케일링 되며 양호한 성능을 발휘했다. (이와 반대되는 주장은 FUD일 뿐이다.)

그런데도 Anthropic은 이를 공개하지 않았다. 대신 Claude 3.5 Opus를 활용해 합성 데이터를 생성하고 보상 모델링(reward modeling)을 수행하여 Claude 3.5 Sonnet을 크게 개선했으며, 여기에 사용자 데이터도 활용했다. 추론 비용은 급격히 변하지 않았지만 모델의 성능은 확연히 개선되었다. 굳이 경제적 관점에서 비효율적인 3.5 Opus를 공개할 이유가 없었던 것이다. 차라리 3.5 Opus로부터 추가적인 사후 학습을 거친 3.5 Sonnet을 공개하는 편이 경제적으로 더 합리적이지 않은가?
더 많은 합성 데이터가 생성되면 더 우수한 모델이 만들어진다. 더 우수한 모델은 더 높은 품질의 합성 데이터를 제공하며, 선호도를 필터링하거나 점수를 매기는 데 더 뛰어난 판단자 역할을 한다. 합성 데이터를 활용하는 과정에는 여러 작은 스케일링 법칙이 내포되어 있으며, 이러한 법칙들이 합쳐져 더 나은 모델을 더 빠르게 개발하도록 이끈다.

### 7. 합성 데이터 예제 ###

#### 7-1. 리젝션 샘플링 ####

합성 데이터가 많이 활용되는 분야 중 하나는 코드 데이터 세트를 생성하는 것이다. 이는 주로 다양한 프로그래밍 작업이나 프롬프트를 시드(seed)로 지정한 후, 모델에게 해당 작업과 관련된 질문을 생성하도록 요청하는 방식으로 이루어진다.
이후 모델에게 가능한 솔루션 세트를 생성하도록 요청한다. 생성된 솔루션 중 관련 테스트를 통과하거나 올바르게 실행될 수 있는 것들만 학습 데이터 세트에 추가되며, 이를 통해 저품질 샘플을 걸러내는 과정은 리젝션 샘플링(Rejection Sampling)이라고 불린다. 리젝션 샘플링은 합성 데이터 생성 과정에서 매우 중요한 요소로, 학습 데이터 세트가 지도 미세조정(SFT) 또는 강화 학습(RL)에 유용할 만큼 충분히 높은 품질을 갖추도록 보장한다. 그러나 이 과정에서 생성된 많은 토큰들이 폐기되므로, 합성 데이터 생성은 많은 연산 자원을 소모한다.

이와 같은 방식으로 미세조정에 활용되는 합성 데이터 세트를 구축하는 방법은 많은 대형 AI 연구소에서 채택하고 있으며, Gemini, GPT, Llama, Claude와 같은 모델의 미세조정에 사용되고 있다. 하지만 리젝션 샘플링은 보기보다 더 복잡할 수 있다. 예를 들어, Llama의 경우 초기 응답이 틀렸을 때 모델에게 답변을 수정하도록 프롬프트를 제공했는데, 두 번째 시도에서 정답을 맞출 확률이 20%에 달했다. 합성 데이터의 유용성을 보여주는 또 다른 사례로, Meta 팀은 파이썬(Python) 코드를 PHP로 변환한 후 구문 분석 및 실행을 통해 품질을 확인하고, 이를 추가적인 SFT 데이터 세트에 포함시켜 공개 PHP 코드의 부족 문제를 해결했다. 이는 합성 데이터를 활용하여 잘 대표되지 않는 영역에서도 유용한 데이터를 신뢰할 수 있고 예측 가능하게 생성할 수 있음을 효과적으로 입증한 사례다.

[원본: Meta] 

#### 7-2. 모델에 의한 판단 ####

또 다른 추세는 다른 대형 언어 모델(LLM)을 판단자로 사용하는 것이다. Meta는 Llama 3의 이전 버전을 리젝션 샘플러로 활용하여, 엄격히 실행 가능한 코드(예: 의사코드)가 아닌 코드를 평가하고, 코드의 정확성과 스타일에 따라 결과를 ‘합격(pass)’ 또는 ’실패(fail)’로 채점하도록 했다. 일부 경우에는 다양한 모델이 동시에 실행되어 모델을 채점하는 방식으로 거절 샘플링이 이루어진다. 이는 결과적으로 인간 데이터보다 비용이 적게 들지만, 이러한 자동화된 판단자 합창(chorus of automated judges)을 구현하는 것은 쉽지 않다.
여기서 주목할 점은, 코드든 아니든 모든 리젝션 샘플링 방식에서 판단자 역할을 하는 모델이 더 우수할수록 생성된 데이터 세트의 품질이 더 높아진다는 것이다. 이 피드백 루프는 Meta가 올해 처음으로 프로덕션 환경에 도입했지만, Anthropic과 OpenAI는 1~2년 전에 이미 이를 활용해 왔다.

#### 7-3. 롱 컨텍스트 데이터셋(Long Context Datasets)![image] ####

합성 데이터 활용의 또 다른 예는 긴 컨텍스트 길이(long context lengths)와 관련된 것이다. 대부분의 데이터가 이미 짧은 컨텍스트 길이로 구성되어 있기 때문에, 모델은 제한된 컨텍스트 길이로 사전 학습(pre-training)을 진행한다. 또한 긴 시퀀스 길이는 메모리에 더 큰 KV 캐시를 유지해야 하므로, 학습 인프라를 배포하는 작업이 기존보다 더 어려워질 수 있다. Gemini, GPT, Claude와 같은 모델들은 초기에는 짧은 시퀀스 길이로 사전 학습된 후, 이후 사후 학습(post-training)을 통해 더 긴 컨텍스트 길이를 추가하는 방식으로 개선된다.
SFT 데이터에서 긴 컨텍스트 예제를 인간이 주석 처리(annotate)하기는 일반적으로 어렵다. 높은 수준의 품질을 제공할 수 있는 충분한 인재를 확보하는 데 한계가 있기 때문이다. 긴 텍스트를 읽고 주석을 다는 작업은 시간이 많이 들고 번거롭다. 이러한 문제를 해결하기 위해 합성 데이터가 유용하고 신뢰할 수 있는 방법으로 떠오르고 있다.
긴 컨텍스트 길이의 합성 데이터를 생성하는 한 가지 방법은 이전 체크포인트에서 모델을 사용해, 현재의 (짧은) 컨텍스트 길이로 나눠진 큰 텍스트 조각을 요약하도록 요청하는 것이다. 이렇게 생성된 요약본, 혹은 경우에 따라 시뮬레이션된 질문과 답변을 포함하는 대화는 합성 데이터 세트를 생성하는 데 활용될 수 있으며, 이후 SFT에 사용될 수 있다.
또 다른 예로는 합성 데이터를 생성해 ‘건초 더미에서 바늘 찾기(needle in haystack)’와 같은 평가 벤치마크를 통과하도록 만드는 경우가 있다. 이 외에도 모델이 확장된 컨텍스트 길이에서 다양한 데이터 부분을 일반화하고 이해하도록 훈련시키기 위한 더 복잡한 유형의 합성 데이터 사례들이 존재한다.

#### 7-4. 강화 학습 ####

강화 학습(Reinforcement Learning, RL)은 대형 언어 모델 경우 모델 정렬(alignment) 및 개선을 위한 주요 방법 중 하나이다. 또한, 강화 학습(RL)은 에이전트가 특정 행동을 수행하거나 특정 결과를 달성하는 데 필요한 행동을 학습하고, 이러한 행동이나 결과에 대해 부여되는 보상을 최대화하도록 훈련되는 과정이다. 강화 학습에서는 두 가지 축(axis)을 고려해야 한다. 하나는 피드백의 출처이고, 다른 하나는 피드백을 어떻게 통합할지에 관한 것이다. 전자는 신호를 어떻게 얻을 것인지에 대한 것이고, 후자는 이러한 신호를 모델을 업데이트하는 데 어떻게 사용할 것인지에 관한 것이다.
강화 학습에서 최적화하려는 대형 언어 모델은 에이전트의 역할을 수행한다. 에이전트는 특정 입력이나 상태(state)에 따라 일련의 행동을 취할 수 있으며, 취한 행동에 따라 서로 다른 보상을 받는다. 우리는 에이전트가 기대되는 누적 보상을 최대화할 수 있는 행동을 학습하도록 하여, 강화 학습 목표에 맞게 에이전트의 행동을 최적화한다.
에이전트가 취할 행동을 결정하고 피드백을 통합하는 주요 접근 방식에는 가치 기반(value-based) 방법과 정책 기반(policy-based) 방법이 있다. 정책 기반 방법에는 직접 선호 최적화(Direct Preference Optimization)와 신뢰 구역 정책 최적화(Trust Region Policy Optimization, TRPO)와 같은 방법이 포함되며, 정책 및 가치 기반 방법을 결합한 행위자-비평가(Actor-Critic) 방법도 있다. 근접 정책 최적화(Proximal Policy Optimization, PPO)는 대표적인 행위자-비평가 모델이며, 이를 기반으로 한 더 복잡한 변형들은 주요 AI 연구소들에서 사용되는 기본적인 강화 학습 방법이다.
반면, 가치 기반 방법은 특정 상태에 도달하는 가치(value)를 계산하고, 가능한 각 상태에 대한 가치를 정의한다. 각 상태는 에이전트가 해당 상태에서 시작할 경우 얻을 수 있는 기대 할인 반환(expected discounted return)에 따라 가치를 부여 받으며, 에이전트는 이용 가능한 각 행동의 가치를 바탕으로 매 단계에서 행동을 결정한다. 과거에는 가치 기반 방법이 강화 학습에서 더 흔히 사용되었지만, 현대의 응용에서는 정책 기반 방법이 훨씬 더 효과적으로 사용되고 있다.

[출처: Huggingface] 

정책 기반 방법(Policy-based methods)에서는 에이전트(Agent)가 정책 함수(policy function)에 의해 작동한다. 이 함수는 주어진 상태(state)에서 취할 수 있는 일련의 행동을 정의하고, 해당 행동들에 확률 분포를 할당한다. 주어진 상태에서 수행할 행동은 결정적(deterministic)일 수 있는데, 이는 각 상태에서 항상 동일한 행동으로 이어진다는 것을 의미한다. 반면, 확률적(stochastic)일 수도 있으며, 이 경우 주어진 상태에서 발생할 수 있는 행동들을 확률 분포로 표현한다. 정책 함수는 기대되는 보상을 최대화하는 행동으로 에이전트를 이끌도록 훈련된다.

[출처: Huggingface] 

강화 학습(RL)에서 정책 기반 방법을 사용할 때, 모델은 결과 보상 모델(Outcome Reward Model, ORM)을 통해 특정 작업의 최종 결과를 평가하여 보상을 결정하거나, 과정 보상 모델(Process Reward Model, PRM)을 통해 특정 프로세스의 개별 단계마다 보상을 평가하여 결정할 수 있다. PRM은 특히 추론(reasoning) 모델을 훈련할 때 유용한데, ORM은 추론 과정이 잘못된 답으로 이어졌다는 사실을 감지할 수 있지만, PRM은 추론 과정의 어느 단계에서 오류가 발생했는지를 알려줄 수 있다.
정책 함수(policy function)가 에이전트가 각 단계에서 수행할 행동을 지시하기 때문에, 이는 추론 과정의 중간 단계에서 에이전트나 모델의 행동을 최적화하는 데 특히 유용한 프레임워크로 작용한다.
ORM과 PRM은 종종 근접 정책 최적화(Proximal Policy Optimization, PPO)에서 사용한다. PPO는 강화 학습에서 자주 사용되는 알고리즘으로, 정책 모델을 반복적으로 개선하여 누적 보상을 최대화하고 대형 언어 모델(LLM)을 주어진 목표에 맞춰 최적화한다. ORM과 PRM을 PPO와 함께 사용하는 것은 현재 커뮤니티에서 주요 관심사로 떠오르고 있는 다단계 추론 모델을 훈련할 때 특히 중요하다. 아래에서는 o1 Pro에 대해 이것이 어떻게 이루어지는지 설명할 것이다.

#### 7-5. 근접 정책 최적화(PPO) ####

근접 정책 최적화(Proximal Policy Optimization, PPO)는 정렬(Alignment)과 미세조정(Fine Tuning) 모두에 사용될 수 있지만, 주로 정렬 과정에서 강화 학습(Reinforcement Learning)에 사용되는 경우가 더 적합하며, 더 자주 활용된다.
PPO에서 정책(Policy)은 에이전트나 모델의 행동을 지시하는 정책 모델의 사용을 의미하고, 근접(Proximal)은 정책을 점진적으로 업데이트하는 알고리즘의 방식을 나타내며, 최적화(Optimization)는 보상 모델로부터 피드백을 제공받아 정책 모델을 개선하고, 이를 통해 기대되는 누적 보상을 최적화하는 과정을 뜻한다.
앞서 주로 정책 기반 방법(Policy-based methods)을 논의했지만, PPO는 정책 기반 방법과 가치 기반 방법(Value-based methods)을 모두 통합해 구현된다. 따라서 PPO는 행위자-비평가(Actor-Critic) 방법을 사용한다고 할 수 있다. 행위자(Actor)는 정책 기반 모델에 의해 구동되며, 주어진 상태에서 어떤 행동을 취할지 결정한다(정책 기반 방법). 비평가(Critic)는 가치 함수(Value Function)를 사용해 행위자가 취한 행동을 평가한다(가치 기반 방법). 행위자와 비평가는 이처럼 반복적인 방식으로 협력하여 작동한다. PPO의 목적 함수(Objective Function)를 최대화하면 정책이 우위 함수(Advantage Function) 값이 더 높은 행동을 선호하는 방향으로 나아가도록 조정된다.

#### 7-6. RLHF ####

인간 피드백을 활용한 강화 학습(Reinforcement Learning with Human Feedback, RLHF)은 대형 언어 모델(LLM)을 정렬(align)하고 유용하게 만드는 주요 기술로, ChatGPT의 폭발적인 성장을 이끈 주요 요인 중 하나다. RLHF는 일반적으로 정책 기반 학습(policy-based learning)을 활용하며, 이는 인간 피드백을 기반으로 학습하는 보상 모델(reward model)을 사용해 모델의 행동 방식을 결정하는 정책(policy)을 업데이트하는 방식이다.
RLHF에서는 인간 주석자(human annotators)가 프롬프트에 대한 응답 샘플을 검토하고, 하나의 응답이 다른 응답보다 선호되는 정도를 순위로 매긴다. 목표는 인간이 어떤 응답을 더 선호하는지에 대한 방대한 데이터를 축적하는 것이다. 이 선호 데이터를 사용해 보상 모델을 학습시키며, 이 모델은 주어진 출력에 대해 평균적인 주석자의 선호도를 예측하도록 훈련된다. 즉, 훈련된 보상 모델은 행위자-비평가(Actor-Critic) 프레임워크에서 비평가(Critic) 역할을 한다.
훈련된 보상 모델은 해당 행동(모델의 출력)이 학습한 인간 선호도와 얼마나 일치하는지, 그리고 평균 행동과 비교해 얼마나 더 나은지 혹은 나쁜지를 평가한다. 이 보상 모델의 피드백은 행위자 모델(Actor model)을 정렬시키는 역할을 하며, 이를 통해 모델이 원하는 정책에 따라 행동(토큰 생성)을 취하도록 한다.

앞서 설명했듯, PPO는 언어 모델의 정책 함수를 반복적으로 업데이트하는 데 사용된다. 이는 학습의 안정성을 유지하면서 정책의 급격한 변화를 방지한다. 대규모 PPO는 AI 연구소에서 도움이 되는 정도(Helpfulness), 진실성(Truthfulness), 안전성(Safety)과 같은 특정 측면에 대해 가중치를 둔 여러 보상 모델을 활용해 수행한다.
일반적으로, RLHF는 실제 최종 사용자가 관심을 가지고 선호 데이터를 제공한 작업에서 모델이 더 나은 성능을 발휘할 수 있도록 한다. Meta의 Llama 2-Chat은 RLHF를 여러 차례 거친 후, 도움이 되는 정도(Helpfulness)와 무해성(Harmlessness)과 같은 요소에서 훨씬 더 뛰어난 성능을 달성했다. 관련 논문은 RL을 통해 모델을 스케일링 할 때 추가적으로 사용된 연산 자원(compute)이 명확한 성과를 가져온다는 점을 보여준다.
인간 생성 피드백 대신 합성 데이터를 활용하고, AI를 피드백의 주요 소스로 더 많이 의존하는 것이 가져오는 잠재적 이점은 더 많은 연산 자원을 사용하는 것을 정당화할 수 있다.

[출처: Meta] 

그러나 RLHF에는 상당한 한계가 있다. 첫 번째로, RLHF 전체 라이프사이클을 실행하는 데 시간이 많이 소요된다. 생성된 다양한 응답을 인간 응답자에게 노출하고 피드백을 받기까지 시간이 필요하며, 이는 일반적으로 AI 회사가 모델 제공 시 프롬프트를 삽입해 피드백을 받거나, 인간 레이블러를 통해 이루어진다.
사용자 기반이 크더라도 선호 데이터(preference data)를 대량으로 수집하는 것은 어렵고 비용이 많이 든다. 예를 들어, Meta는 Llama 2를 위해 선호 데이터를 수집하는 데 1,000만~2,000만 달러를 지출했으며, 이는 연산 시간(compute time) 비용보다 더 많은 금액이다.
RLHF는 본질적으로 확장성이 어려운 방법이다. 특히 기존 데이터가 많지 않은 영역에서는 더욱 그렇다. 또한 인간 주석 작업(human annotation)은 비용이 많이 든다. 이러한 이유로 많은 AI 회사들이 학습 중에 AI 피드백을 활용한 강화 학습(Reinforcement Learning with AI Feedback, RLAIF)으로 전환하고 있다.
더 큰 AI 회사들은 이 분야에서 명확한 이점을 가지고 있다. Claude, Gemini, ChatGPT는 모두 호스팅하는 모델의 응답에 대해 사용자들이 피드백을 제공하도록 요청한다. 예를 들어, ChatGPT는 때때로 두 개의 응답 중 어떤 것을 선호하는지 선택하라고 명시적으로 요청한다. 이는 사실상 최상의 피드백 소스를 무료로(사용자에게서 직접) 수집하는 방법이다. OpenAI는 3억 명 이상의 방대한 고객 기반을 보유하고 있기 때문에, 모델을 개선하기 위해 많은 피드백을 수집할 수 있다.

사용자 수가 적거나 사용자 피드백 제공에 적합하지 않은 플랫폼을 운영하는 제공업체는 PPO 대신 DPO와 같은 다른 방법에 의존해야 한다. 직접 선호 최적화(Direct Preference Optimization, DPO)는 RLHF와 함께 자주 논의되는 기술이지만, 대부분 이를 강화 학습 기법으로 엄밀히 분류하지는 않는다.
DPO는 보상 모델을 훈련하는 과정을 완전히 생략하고, 대신 최적화를 통해 정책(policy)을 직접 조정하여 인간 선호 데이터를 기반으로 모델이 선호되는 출력을 생성하도록 유도하는 확률을 극대화한다. 이 최적화는 현재 모델과 기준 모델(일반적으로 미세조정 이전의 동일 모델) 간의 확률 비율을 비교하는 이진 교차 엔트로피 손실(binary cross-entropy loss)을 활용한다. DPO는 모델이 선호되는 응답을 학습하도록 하면서도 기준 모델의 행동에 가깝게 유지되도록 보장한다.
DPO의 더 간단한 접근 방식은 완전한 보상 모델을 사용하는 RLHF에 비해 비슷하거나 더 나은 결과를 달성할 수 있으며, 크래시(crash)가 덜 발생하고 구현이 더 쉽다. 이 접근 방식의 장점을 보여주는 대표적인 사례는 Llama 3가 RLHF를 거치지 않고 DPO를 적용한 경우이다. Meta는 Llama 3의 경우, DPO가 PPO보다 더 효과적이고 안정적이며, 연산 자원도 덜 소모된다는 점을 발견했다. 그러나 DPO를 사용하는 경우 선호 데이터 세트의 품질이 매우 중요하다. 따라서 이 데이터를 수집하고 처리하는 방식에 대해 더 많은 주의와 신경을 기울일 필요가 있다.

[출처: Meta]

Meta는 결국 다른 연구소들이 이미 알고 있던 교훈을 깨달았다. DPO는 PPO만큼 확장성이 뛰어나지 않으며, 사후 학습(post training)을 계속 개선하기 위해서는 RLAIF로 전환해야 한다는 점이다. 이는 최신 LLAMA 3.3의 출시에서 입증되었다.

#### 7-7. RLAIF ####

인간 피드백에 의존해 보상 모델을 훈련하는 대신, AI 피드백을 활용한 강화 학습(Reinforcement Learning with AI Feedback, RLAIF)은 인간 피드백을 다른 모델로 대체한다. 보상 모델은 AI가 생성한 피드백을 기반으로 훈련되며, 이는 일반적으로 특정 완성(completion)을 평가하고 이에 따라 보상을 결정하는 형태의 스코어링 모델(scoring model)이나 알고리즘을 사용한다.

[출처: RLAIF vs RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback]

전반적으로, RLAIF는 RLHF와 본질적으로 크게 다르지 않지만, 극적인 차이를 만들어낸다. 주석(annotation)을 빠르게 생성할 수 있으며, 합성적으로 생성된 프롬프트를 사용해 강화 학습 중인 모델이 추가 데이터나 학습이 필요한 영역에서 훈련을 받을 수 있도록 한다.
RLAIF는 수학, 과학, 일반 상식과 같은 일반적인 작업에 대한 피드백을 제공하는 것 외에도, 윤리적 딜레마, 문화적 규범, 사회적 상호작용과 같은 더 미묘한 상황을 다루기 위한 피드백을 다른 대형 언어 모델(LLM)을 통해 빠르게 생성하고 순위를 매길 수 있다. 이를 통해 모델이 정렬(alignment)되어야 하는 주제의 범위를 넓힐 수 있을 뿐만 아니라, 인간 피드백을 기다리지 않고도 해당 주제에 대한 학습을 빠르게 확대할 수 있다.

RLAIF의 독특한 활용 사례 중 하나는 Anthropic의 헌법적 AI(Constitutional AI) 이다. 헌법적 AI는 두 단계로 작동한다. 첫 번째 단계에서는 기본 모델(base model)이 인간이 작성한 헌법 원칙(constitutional principles)을 기반으로 자신의 출력을 비판하고 수정한다. 이 초기 응답은 유해하거나 도움이 되지 않을 수 있다. 이후 헌법의 다양한 원칙을 사용해 응답을 지속적으로 수정한다. 이렇게 생성된 수정된 응답과 프롬프트 쌍은 지도 미세조정(Supervised Fine-Tuning, SFT)을 통해 모델을 미세조정 하는 데 사용되는 데이터 세트를 만든다.
헌법적 AI(Constitutional AI) 프로세스의 두 번째 단계는 RLHF와 유사하지만, 무해성(harmlessness)에 대한 피드백을 제공하는 인간 선호 데이터를 사용하지 않는다는 점에서 차이가 있다. AI는 이전 단계의 모델에서 생성된 응답 쌍을 헌법 원칙에 따라 평가하며, 이 원칙은 사실상 여러 보상 모델과 유사한 역할을 한다. AI가 생성한 무해성에 대한 선호 데이터는 인간 피드백 데이터를 활용한 유용성(helpfulness)에 대한 선호 데이터와 결합되어 **하이브리드 선호 모델(hybrid preference model)**을 훈련하는 데 사용된다. (여기서 “하이브리드”는 인간 데이터를 포함한다는 의미다.) 마지막으로, 첫 번째 단계의 모델은 이 선호 모델을 보상 신호로 사용해 RL을 통해 미세조정된다.
이 접근 방식에서 가장 주목할 만한 점은 다양한 도메인에서 확장 가능하다는 것이다. 예를 들어, 무해성을 식별하는 능력 외에도 더 과학적으로 정확한 응답을 평가할 수 있는 모델이 있다면, 해당 모델을 활용해 과학적으로 정확한 응답을 최적화할 수도 있다.

[출처: Anthropic Constitutional AI: Harmlessness from AI Feedback]

강화 학습(RL)은 연쇄적 사고(Chain of Thought, CoT)를 사용하는 추론 모델(reasoning models)을 개발하는 데에도 핵심적인 역할을 한다.

#### 7-8. 추론 모델과 연쇄적 사고(CoT) ####

수학은 공학, 건설, 시스템 설계의 근본적인 논리와 추론의 기초다. 수학은 모델을 미세조정 할 때 중요한 분야로 주목받고 있는데, 이는 모델 훈련자가 고난도의 복잡한 프롬프트를 충분히 확보하지 못하고 있기 때문이다. 이 문제를 극복하는 한 가지 방법은 고도로 숙련된 전문가를 고용해 프롬프트를 작성하거나, 내부적으로 생성하는 것이다. 수학 문제를 효과적으로 해결하려면 모델이 배울 수 있는 명확하고 올바른 연쇄적 사고(Chain of Thought, CoT)가 필요하다.
일부 수학적 기능은 코드 인터프리터 접근(Code Interpreter Access)과 같은 도구를 통해 개선될 수 있다. 이를 통해 모델이 파이썬(Python)과 같은 언어로 코드를 생성하고 실행하며 일부 수학 문제를 해결할 수 있도록 돕는다. 그러나 코드만으로는 많은 문제, 특히 가장 어려운 수학 문제를 해결하기에 충분하지 않다. 현재 복잡한 수학 문제를 해결할 수 있는 추론 모델(reasoning model)을 훈련하는 데 막대한 노력이 집중되고 있다.

모델은 기본적으로 연쇄적 사고를 생성하도록 프롬프트를 받을 수 있지만, 각 단계에서 발생한 오류가 최종 해답까지 누적되어 잘못된 결과로 이어질 수 있기 때문에 결과가 신뢰할 수 없는 경우가 많다. 그러나 o1 Pro는 이를 방지하기 위한 여러 안전장치를 갖추고 있다. 또 다른 도전 과제는 최신 모델조차도 불확실한 상황에서는 정보를 환각(hallucinate)하거나 조작(fabricate)할 수 있다는 점이다. 이는 추론 단계 중 하나에서 발생한 오류를 쉽게 확대시킬 수 있다.
연쇄적 사고(Chain of Thought)를 활용해 추론을 수행하도록 정렬된 모델은 위에서 언급한 많은 과제를 해결할 수 있다. 이 접근법에서는 강화 학습(Reinforcement Learning)을 사용해 모델의 행동을 연쇄적 사고 방식에 맞게 정렬하고, 이를 개선하기 위해 여러 독립적인 모델 및 대형 언어 모델(LLM)을 활용한다.
먼저 논의할 독립적인 LLM은 생성기(Generator)이다. 생성기는 여러 단계를 거쳐 논리적으로 도출된 솔루션을 생성하도록 훈련된다. 생성기는 보통 기본 LLM과 분리된 상태로 작동하며, 이러한 추론 단계를 생성하는 작업에 특화되어 미세 조정된다. 반면, 기본 LLM은 주로 일반적인 작업을 처리하도록 미세 조정된다.
두 번째는 검증 모델(Verifier Model)이다. 검증 모델은 생성기가 만들어낸 솔루션이 올바른 지 여부를 평가하고 이에 따른 보상을 제공하는 역할을 한다.
검증 모델은 인간 주석(human annotation), 자동화된 프로세스 주석(automatic process annotation), 또는 자동 검증기(automatic verifiers)를 통해 훈련될 수 있다. 예를 들어, OpenAI의 논문 Let’s Verify Step by Step에서는 PRM800K 프로세스 수퍼비전 데이터세트(PRM800K Process Supervision Dataset)를 소개되었다. 이 데이터세트는 12,000개의 수학 데이터세트(MATH Dataset) 문제에 대한 75,000개의 솔루션에서 생성된 800,000개의 프로세스 단계를 인간 데이터 레이블러가 주석 처리한 것이다. 이 데이터세트는 논문에서 논의된 바와 같이 생성기 출력에 기반하여 작성되었다.

[출처: Let’s Verify Step by Step]

이러한 주석(annotation)을 수집하는 비용은 결코 사소하지 않다. 원래의 Math 논문에서는 대학생 몇 명이 한 시간 동안 20개의 문제를 풀도록 했는데, 점수는 40%에서 90% 사이였으며, 90%를 기록한 학생은 국제수학올림피아드(IMO) 금메달을 세 번이나 수상한 사람이었다. OpenAI의 논문에서는 비용 문제를 언급하며, PRM 지향 데이터세트를 인간이 주석 처리하여 ORM 지향 데이터세트만큼 대규모로 구축하는 것은 현실적으로 불가능하다고 지적했다. 이는 공정한 비교(apples-to-apples comparison)를 수행하기 어렵게 만든다.

대안으로는 자동화된 프로세스 주석(automatic process annotation)을 사용하거나 자동 검증기(automatic verifiers)를 활용하는 방법이 있다. 자동 검증기는 이상적으로는 주어진 문제의 솔루션이 올바른 지 여부를 빠르고 쉽게 확인할 수 있는 시스템이나 모델을 의미한다. 예를 들어, 코드의 경우 실제로 코드를 실행하여 원하는 결과를 생성하는지 테스트하는 방식이 될 수 있다. 수학에서는 특정 함수를 평가하거나 LEAN과 같은 증명(prover)을 사용해 정답 여부를 확인할 수 있다. 그러나 자동 검증기를 사용하는 것이 생각만큼 “자동화”된 것은 아닐 수 있다. 외부 시스템에 대한 의존성이 생기면 추가적인 오버헤드가 발생하여 학습 성능이 저하될 수 있으며, 자동 검증기가 실행에 시간이 걸리는 경우도 있기 때문이다.

자동화된 프로세스 주석은 단계별 프로세스 주석을 생성하는 방법이다. 중간 단계를 사람이 평가하는 대신 컴플리터(Completer)를 사용하여 여러 경로의 추론 단계를 생성한다. Math-Shepherd 논문에서는 자동화된 프로세스 주석을 사용했는데, 여러 경로를 생성한 후 이 경로를 평가하는 방식으로 다음 두 가지를 활용했다:

      * Hard Estimation: 최종 정답으로 이어지는 경우 해당 단계를 올바른 추론 단계로 표시
      * Soft Estimation: 해당 단계가 정답으로 이어지는 빈도를 기반으로 점수를 부여.
      
이 방법들은 인간 주석의 필요성을 줄이고 학습을 보다 효율적으로 진행할 수 있도록 돕는다.

[출처: Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations]

네 번째 모델은 보상 모델(Reward Model)로, 프로세스 주석(process annotation) 레이블을 기반으로 훈련된다. 앞서 설명을 요약하자면, 보상 모델에는 두 가지 유형이 있다. 결과를 기준으로 보상을 제공하는 결과 보상 모델(Outcome Reward Model, ORM)과 프로세스를 기준으로 보상을 제공하는 프로세스 보상 모델(Process Reward Model, PRM)이다. ORM은 일반적으로 모델이 제공하는 다양한 답변을 순위로 매기고, 가장 높은 순위를 받은 답변을 선택하는 방식으로 작동한다. 이에 반해 PRM은 연쇄적 사고(Chain of Thought) 과정의 각 단계를 평가하고 점수를 부여하며, 이 점수를 기반으로 보상을 제공한다. 이러한 이유로 PRM은 연쇄적 사고 모델을 훈련할 때 선호된다. Let’s Verify Step by Step 논문에서는 PRM이 ORM보다 더 강력한 성과를 보여준 바 있다. 그렇긴 하지만 OpenAI는 여전히 ORM에 더 많이 의존하고 있다.

[출처: Let’s Verify Step by Step]

Math-Shepherd에서는 단계별 근접 정책 최적화(Proximal Policy Optimization, PPO)를 활용한 강화 학습(Reinforcement Learning)을 통해 최종 대형 언어 모델(LLM)에 원하는 연쇄적 사고(Chain of Thought) 행동을 학습시키고 강화한다.

#### 7-9. 추론 시점 스케일링 ####

OpenAI의 o1 프리뷰 출시로 인해 업계는 새로운 스케일링 법칙의 부상에 주목하게 되었다. 이는 테스트 시점 연산(test-time compute, 즉 추론 시점의 연산)이 많아질수록 더 나은 답변을 얻을 수 있다는 것으로, 이 스케일링 차원을 활용하려는 노력이 중요한 전환점에 도달했음을 보여준다. 전통적인 대형 언어 모델(LLM)은 간단한 질문이든 어려운 질문이든, 입력된 질문에 대해 중간 단계를 추적하지 않고 연속적으로 토큰을 생성하며, 답변에 도달했다고 판단되면 결과를 출력한다.

반면, 앞서 설명했듯이 추론 모델(Reasoning Models)은 응답을 사용자에게 전달하기 전에 연쇄적 사고(Chain of Thought)라 불리는 일련의 단계로 나눠서 처리한다. 추론 모델은 비논리적인 결론에 도달했을 경우, 오류가 발생했거나 특정 접근 방식이 막다른 길에 이르렀음을 인지하고 이전 단계를 다시 검토함으로써 추론 과정을 올바른 방향으로 되돌릴 수 있다.
추론 모델 출시로 인해 두 가지 중요한 의미가 도출된다. 첫째, 코딩, 수학, 과학과 같은 고난도 평가에서 모델 성능이 눈에 띄게 향상될 가능성이 커졌다. 둘째, 이러한 성능 개선이 테스트 시점 연산(test-time compute)과 함께 확장될 수 있다는 점이 대형 언어 모델(LLM)에도 강력히 적용된다는 사실이 확인되었다.

[Source: OpenAI] 

테스트 시점 스케일링(test-time scaling)은 새로운 개념이 아니다. 바둑이나 포커 같은 게임에서는 테스트 시점 연산을 확장하는 개념이 이미 오래전부터 존재해왔다. 예를 들어, 딥마인드(DeepMind)의 바둑 시스템인 AlphaGo는 테스트 시점에 몬테카를로 트리 탐색(Monte Carlo Tree Search)을 사용해 최적의 수를 결정한다. 만약 추론 시점에서 이러한 탐색 기능이 제거된다면, AlphaGo의 Elo 등급은 약 5,200에서 3,000으로 하락하며(참고로 인간 최고 수준은 약 3,800), 추론 시점 연산 덕분에 AlphaGo는 바둑에서 초인적인 성과를 거둘 수 있었다.
더 많은 연산 자원을 활용하면, 추론 모델은 더 많은 단계를 사고할 수 있어 올바른 답에 도달할 가능성이 높아진다. 현재 추론 모델의 추론 능력은 추론 시스템(inference system)의 한계에 의해 병목 현상을 겪고 있다. 추론 모델이 요구하는 긴 컨텍스트 길이는 메모리와 연산 자원 요구량을 상당히 증가시키기 때문이다.

이는 추론 시스템 운영자들이 컨텍스트 길이를 합리적인 수준으로 유지하고 비용을 낮추기 위해 연쇄적 사고(Chain of Thought)의 길이를 제한하고 있다는 것을 의미한다. 이렇게 해야만 합리적인 토큰 대 토큰 지연(latency)으로 경제적인 사용자 수를 감당할 수 있다. 결과적으로, 현재의 추론 모델은 마치 한쪽 손이 묶인 상태에서 작동하고 있으며, GB200 NVL72와 같은 더 강력한 추론 시스템이 시장에 등장하면 성능이 크게 확장될 수 있을 것이다. 비용이 경제적으로 가능해지면, o1 모델이 자신의 추론 체인의 길이와 사용 연산량을 조정하도록 허용하는 것이 테스트 시점 연산 스케일링을 활용하는 중요한 기술이 될 것이다.

[Source: OpenAI]

평가(evals)와 아래의 그래프에서 알 수 있듯이, 한 번의 시도로도 GPT-4o는 다른 모델들을 능가한다. 테스트 시점 연산을 스케일링하는 가장 단순한 방법은 단순히 동시에 실행되는 샘플의 수를 늘리는 것이다. 이는 사실상 무한 원숭이 정리(infinite monkey theorem)를 활용하는 방식이다. 논문 Large Language Monkeys는 단순히 반복 샘플링(repeated sampling)을 통해 추론 시점 연산을 스케일링할 수 있으며, 이를 통해 훨씬 더 나은 결과를 얻을 수 있음을 보여준다.

[Source: Large Language Monkeys: Scaling Inference Compute with Repeated Sampling]

이는 아마도 검색(search)을 수행하는 가장 기본적인 방법 중 하나로 볼 수 있다. 더 많은 샘플을 생성하면 더 넓은 범위를 다룰 수 있는데, 이는 생성된 샘플 중 하나라도 정답에 도달하는 경우(예: pass@k)로 정의된다. 단순히 이러한 작은 모델들이 한 문제를 여러 번 생각하도록 하는 것이 더 정확하면서도 비용 효율적일 수 있다는 주장도 있을 수 있다. 그러나 우리가 비유적으로 “셰익스피어 전집”을 완성했음을 확인하려면 효과적인 검증기(verifier)가 필요할 것이다.

“It was the best of times, it was the blurts of times”
[출처: The Simpsons]

#### 7-10. 검색을 통한 추론 연산 스케일링 ####

검색(Search)은 OpenAI o1에서는 활용되지 않았지만 o1 Pro에서 활용되는 또 다른 스케일링 차원이다. o1은 테스트 시점(즉, 추론 중)에 여러 경로의 추론을 평가하거나 어떤 형태의 검색도 수행하지 않는다. Sasha Rush의 Speculations on Test-Time Scaling (o1) 비디오는 검색과 추론 모델과 관련된 다른 주제들에 대해 유용한 논의와 설명을 제공한다.
자기 일관성(Self-Consistency) / 다수결(Majority Vote)은 이러한 검색 방법론 중 하나로, 프롬프트를 모델에 여러 번 통과시켜 다수의 응답을 생성한 다음, 주어진 샘플 수에서 가장 자주 등장하는 응답을 선택하여 정답으로 결정하는 방식이다.

[출처: Sasha Rush]

Best-of-N Sampling은 특정 프롬프트에 대해 N개의 솔루션을 생성한 다음, 검증 모델(verifier model)을 사용해 올바른 답으로 이어진 연쇄적 사고(Chain of Thought)를 식별하는 아이디어다. 이 방법은 일반적으로 검증이 가능한 영역(예: 스도쿠와 같은 문제, 에세이는 제외)에 제한되며, 검증 모델의 효과에 의해 그 성과가 좌우된다.

[출처: Sasha Rush]

몬테카를로 롤아웃(Monte Carlo roll-outs)은 Best-of-N 방법을 기반으로 한 기법이다. 이 기법에서는 특정 중간 단계를 평가하기 위해, 해당 중간 단계에서 시작하여 연쇄적 사고(Chain of Thought)를 완성하는 여러 경로를 생성한다. 이러한 평가는 해당 단계를 계속 진행할지, 아니면 잠재적인 다른 단계로 넘어갈지를 결정하는 데 도움을 주며, 전체적인 사고 과정의 질을 개선할 수 있다.
이제 강화 학습(RL), 합성 데이터(Synthetic Data), 연쇄적 사고(Chain of Thought), 추론 시점 연산(Inference Time Compute) 및 기타 개념에 대해 논의했으니, OpenAI가 o1과 o1 Pro에서 학습 및 추론 중에 어떤 작업을 수행했는지 살펴보자. o1의 구성은 독특하며, 앞서 논의한 논문들과는 다르다. 또한 추론 시점 연산의 경제성(tokenomics), 즉 비용, KV 캐시 스케일링(KV Cache scaling), 배치 처리(batch processing) 등을 포함해 자세히 살펴볼 것이다. 마지막으로, OpenAI가 Orion에서 앞으로 어떤 작업을 하고 있는지, 그리고 이를 실패로 보는 서사가 왜 부정확한지를 설명할 것이다.

|레피런스|
|------|
| https://semianalysis.com/2024/12/11/scaling-laws-o1-pro-architecture-reasoning-training-infrastructure-orion-and-claude-3-5-opus-failures/![image](https://github.com/user-attachments/assets/77992440-5599-43a4-8acc-e33b8685c214) |
























