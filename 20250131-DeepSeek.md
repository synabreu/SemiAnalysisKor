# DeepSeek 논쟁: 전체 비용, 실제 훈련 및 폐쇄 모델 마진 영향 측면에서 본 중국 리더쉽 #

## 전체 요약: H100 가격 급등, 보조금 지원 추론 가격, 수출 통제, MLA ##

### 1. 전 세계를 강타한 DeepSeek ### 

지난 한 주 동안, DeepSeek은 전 세계적으로 가장 뜨거운 화제가 되었다. 현재 DeepSeek의 일일 트래픽은 Claude, Perplexity, 심지어 Gemini보다도 훨씬 높은 수준에 도달했다. 그러나 AI 업계를 면밀히 관찰해 온 사람들에게 이는 완전히 새로운 소식은 아니다. [SemiAnalysis는](https://x.com/dylan522p/status/1819431961368129554?mx=2) 이미 몇 달 전부터 [DeepSeek에 대해](https://x.com/dylan522p/status/1859302712803807696) [논의해](https://x.com/dylan522p/status/1875594509339521414) [왔다.](https://semianalysis.com/2024/05/07/openai-is-doomed-et-tu-microsoft/) DeepSeek 자체는 새로운 기업이 아니지만, 현재의 광적인 열풍은 분명 새롭다. 

SemiAnalysis는 오랫동안 DeepSeek이 매우 뛰어난 기술력을 보유하고 있음에도 불구하고, 미국의 대중은 이에 대해 큰 관심을 보이지 않았다고 주장해 왔다. 하지만 이제 세계가 DeepSeek에 주목하기 시작하자, 현실을 제대로 반영하지 못하는 과장된 열풍이 형성되었다. SemiAnalysis는 한 달 전과 비교해 완전히 뒤바뀐 내러티브를 강조하고 싶다. 당시에는 스케일링 법칙이 깨졌다는 주장이 나왔지만, 우리는 이 신화를 불식시켰다.(*) 이제는 알고리즘 개선 속도가 지나치게 빠르다는 우려가 나오며, 이것이 NVIDIA 및 GPU 산업에 부정적인 영향을 미칠 수도 있다는 의견까지 제기되고 있다. 

현재 내러티브는 DeepSeek이 너무 효율적이어서 더 많은 컴퓨팅 자원이 필요하지 않으며, 모델의 변화로 인해 모든 인프라가 과잉 용량 상태에 놓였다는 것이다. 그러나 Jevons 역설 역시 과장된 측면이 있지만, 현실에 더 가까운 것은 Jevons 역설이다. 이미 모델들은 실제 수요를 유발했으며, 이는 H100과 H200의 가격에 가시적인 영향을 미치고 있다. 

### 2. DeepSeek 과 High-Flyer의 관계 ###

High-Flyer는 중국의 헤지펀드로, 트레이딩 알고리즘에 AI를 조기에 도입한 선구적인 기업이다. 이들은 AI가 금융 분야뿐만 아니라 다양한 산업에서 지닌 잠재력을 일찍이 인식했으며, 스케일링(Scaling)의 중요성을 간파했다. 그 결과, 지속적으로 GPU 공급을 확대해 왔다.

수천 개의 GPU 클러스터를 활용한 모델 실험을 거친 후, High-Flyer는 2021년 수출 제한이 적용되기 전에 10,000개의 A100 GPU에 투자했다. 이 결정은 큰 성과를 거두었다.
High-Flyer가 기술을 발전시켜 나가면서, 보다 집중적으로 AI 역량을 강화할 필요성을 느끼게 되었다. 이에 따라 2023년 5월, "DeepSeek"을 스핀오프(분사) 하기로 결정했다. DeepSeek은 AI 연구 및 개발에 보다 집중하기 위한 목적으로 설립되었다.

당시 AI 관련 비즈니스 모델의 부재로 인해 외부 투자자들의 관심이 크지 않았으나, High-Flyer는 자체 자금으로 DeepSeek을 지원했다. 현재 High-Flyer와 DeepSeek은 인적 및 컴퓨팅 자원을 공동으로 활용하며 긴밀히 협력하고 있다.

DeepSeek은 이제 진지하고 체계적인 노력을 기울이는 대규모 프로젝트로 성장했으며, 일부 언론에서 주장하는 것처럼 결코 단순한 "사이드 프로젝트"가 아니다. 우리는 수출 통제 조치를 고려하더라도, DeepSeek이 GPU 투자에 5억 달러(미화) 이상을 지출했을 가능성이 높다고 확신하고 있다.

### 3. 현재 GPU 상황 ###

SemiAnalysis는 해당 기관이 약 50,000개의 Hopper GPU에 접근할 수 있다고 보고 있다. 이는 일부에서 주장하는 50,000개의 H100과는 다르다. NVIDIA는 다양한 규제를 준수하기 위해 H100의 여러 변종(예: H800, H20)을 제작했으며, 현재 중국 모델 제공자에게는 H20만이 제공되고 있다. 참고로 H800은 H100과 동일한 계산 성능을 갖추고 있지만, 네트워크 대역폭은 더 낮다.

DeepSeek는 약 10,000개의 H800과 10,000개의 H100에 접근할 수 있는 것으로 보인다. 또한, DeepSeek는 추가로 많은 H20에 대한 주문을 보유하고 있으며, NVIDIA는 지난 9개월 동안 중국 전용 GPU를 100만 개 이상 생산한 것으로 알려져 있다. 이 GPU들은 High-Flyer와 DeepSeek 간에 공유되며, 어느 정도 지리적으로 분산되어 사용되고 있다. 이들은 트레이딩, 추론, 학습, 연구 등 다양한 용도로 활용되고 있다. 보다 구체적이고 상세한 분석은 해당 기업의 [Accelerator Model을](https://semianalysis.com/accelerator-industry-model/) 통해 확인할 수 있다.

SemiAnalysis 분석에 따르면, DeepSeek의 전체 서버 자본 지출(CapEx)은 약 16억 달러에 달하며, 이 클러스터를 운영하는 데 드는 비용은 약 9억 4,400만 달러로 상당한 규모를 자랑한다. 마찬가지로, 모든 AI 연구소와 Hyperscaler들은 자원 중앙화가 어려워 개별 훈련 작업에 할당하는 것보다 다양한 작업을 위해 훨씬 더 많은 GPU를 보유하고 있다. X.AI는 모든 GPU가 하나의 장소에 모여 있는 독특한 AI 연구소이다.

DeepSeek은 인재를 중국에서만 채용하며, 이전 경력에 관계없이 능력과 호기심을 중요하게 여긴다. DeepSeek은 북경대(PKU)와 절강대(Zhejiang)와 같은 명문 대학에서 채용 행사를 자주 개최하며, 이곳에서 많은 직원들을 뽑았다. 직원의 역할은 반드시 사전 정의되어 있지 않으며, 채용된 사람들에게는 유연성이 주어지며, 구인 광고에는 사용 제한 없는 10,000개 이상의 GPU에 접근할 수 있다는 내용도 포함되어 있다. 이들은 매우 경쟁력이 있으며, 유망한 후보자에게 130만 달러 이상의 연봉을 제안한다고 전해지며, 이는 Moonshot과 같은 다른 대형 중국 기술 기업 및 AI 연구소보다 훨씬 높은 금액이다. 현재 직원 수는 약 150명이며, 빠르게 성장하고 있다.

역사적으로 볼 때, 자금이 잘 지원된 작은 스타트업이 종종 가능한 한계를 뛰어넘을 수 있다. DeepSeek은 구글과 같은 대기업의 관료주의가 없고, 자체 자금으로 운영되기 때문에 아이디어를 빠르게 실현할 수 있다. 그러나 구글처럼 DeepSeek도 대부분 자체 데이터 센터를 운영하며, 외부 파트너나 협력 업체에 의존하지 않는다. 이는 실험을 위한 더 많은 기회를 제공하고, 전체 스택에서 혁신을 이룰 수 있게 한다. SemiAnalysis는 DeepSeek가 오늘날 "공개 가중치(Open Weight)" 연구소 중 가장 우수하다고 믿으며, Meta의 Llama, Mistral 등 다른 경쟁자를 능가한다고 평가한다.

### 4. DeepSeek의 비용과 성능 ###

DeepSeek의 가격과 효율성은 이번 주에 큰 관심을 끌었으며, 주요 헤드라인은 DeepSeek V3의 “600만 달러” 훈련 비용이었다. 이는 잘못된 정보다. 이는 제품의 자재 목록에서 특정 부분을 지적하고 그것을 전체 비용으로 잘못 해석하는 것과 같다. 사전 훈련 비용은 전체 비용에서 매우 작은 부분에 해당한다.

### 4-1. 훈련 비용 ###
SemiAnalysis는 사전 훈련에 사용된 숫자가 모델에 실제로 지출된 총 비용이라고 보지 않는다. 회사 역사 전체를 통틀어 하드웨어에 지출된 비용은 5억 달러 이상일 것이라고 확신한다. 새로운 아키텍처 혁신을 개발하기 위해, 모델 개발 과정에서는 새로운 아이디어와 아키텍처, 그리고 ablation 실험  등에 상당한 비용이 소요된다. DeepSeek의 주요 혁신 중 하나인 Multi-Head Latent Attention 은 개발하는 데 몇 달이 걸렸으며, 한 팀의 인력 시간과 GPU 사용 시간이 모두 소요되었다.

논문에 언급된 600만 달러 비용은 사전 훈련 실행에서 발생한 GPU 비용에 불과하며, 이는 모델 전체 비용의 일부에 불과하다. 여기에는 R&D 비용과 하드웨어 자체의 총 소유 비용(TCO) 등 중요한 부분들이 제외되어 있다. 참고로, Claude 3.5 Sonnet의 훈련 비용은 수천만 달러에 달했으며, 만약 그 비용이 Anthropic이 필요로 하는 전체 비용이었다면, 구글로부터 수십억 달러, 아마존으로부터 수백억 달러를 모금하지 않았을 것이다. 이는 그들이 실험, 새로운 아키텍처 개발, 데이터 수집 및 정제, 직원 급여 지급 등 다양한 추가 비용을 감안해야 하기 때문이다. 그렇다면, DeepSeek이 어떻게 이렇게 대규모 클러스터를 보유할 수 있었을까? 수출 통제의 지연이 그 핵심이며, 이에 대해서는 아래의 수출 섹션에서 다룰 예정이다.

        * Ablation 실험: 모델이나 시스템의 특정 구성 요소를 하나씩 제거하거나 변경해 보면서, 그 요소가 전체 성능이나 결과에 미치는 영향을 분석하는 실험 방법을 말한다. 이를 통해 각 요소의 기여도를 파악하고, 어떤 부분이 모델의 성능에 중요한지 또는 불필요한지를 평가할 수 있다.

        * Multi-Head Latent Attention은 DeepSeek가 개발한 주요 혁신 중 하나로, 일반적인 Transformer의 Multi-Head Attention 개념을 확장하거나 변형한 형태임. 기본적으로 Multi-Head Attention은 입력의 여러 부분 간의 상호관계를 동시에 여러 ‘어텐션 헤드’를 통해 학습하여, 다양한 특징을 동시에 포착하는 역할을 한다. 여기서 "Latent"라는 단어가 추가된 것은, 어텐션 계산이 단순히 입력 토큰 간의 관계뿐만 아니라, 모델의 잠재 공간(latent space)에서 추출된 은닉 표현들 간의 관계에도 초점을 맞춘다는 의미로 해석할 수 있다. 즉, Multi-Head Latent Attention은 모델 내부의 잠재적 특징들을 여러 어텐션 헤드를 통해 병렬로 분석함으로써, 다양한 관점에서 데이터를 해석하고 보다 풍부한 정보를 학습할 수 있도록 도와주는 기법이다.

### 4.2 DeepSeek V3의 성능 – o1과 격차 해소 ###   

V3는 의심할 여지없이 인상적인 모델이지만, “인상적”이라는 표현이 무엇과 비교할 때 인상적인지를 강조할 필요가 있다. 많은 사람들이 V3를 GPT-4o와 비교하며 V3가 4o보다 우수한 성능을 보인다고 강조한다. 이는 사실이지만, GPT-4o는 2024년 5월에 출시되었다. AI는 빠르게 발전하며, 2024년 5월은 알고리즘 개선 측면에서 이미 오래 전의 일이 되었다. 게다가 일정 시간이 지난 후 비슷하거나 더 강력한 능력을 적은 계산 자원으로 달성하는 것은 놀라운 일이 아니다. 추론 비용의 급감은 AI 발전의 대표적인 특징이다.

V3는 의심할 여지없이 인상적인 모델이지만, “인상적”이라는 표현이 무엇과 비교할 때 인상적인지를 강조할 필요가 있다. 많은 사람들이 V3를 GPT-4o와 비교하며 V3가 4o보다 우수한 성능을 보인다고 강조한다. 이는 사실이지만, GPT-4o는 2024년 5월에 출시되었다. AI는 빠르게 발전하며, 2024년 5월은 알고리즘 개선 측면에서 이미 오래 전의 일이 되었다. 게다가 일정 시간이 지난 후 비슷하거나 더 강력한 능력을 적은 계산 자원으로 달성하는 것은 놀라운 일이 아니다. 추론 비용의 급감은 AI 발전의 대표적인 특징이다.

지금까지 이러한 패턴에서 SemiAnalysis가 목격한 바에 따르면, AI 연구소들은 투자하는 비용 대비 더 높은 지능을 얻기 위해 절대적인 금액을 더 많이 지출하고 있다. 알고리즘 발전은 연간 4배의 향상을 보인다는 추정이 있으며, 이는 매년 동일한 성능을 달성하기 위해 필요한 계산량이 4배 줄어든다는 의미이다. Anthropic의 CEO인 Dario는 알고리즘 발전 속도가 더 빠르며 10배의 개선 효과를 가져올 수 있다고 주장한다. GPT-3 수준의 추론 비용에 관해서는 비용이 1,200배 감소했다.

GPT-4의 비용을 조사해 보면, 비용 감소가 유사하게 나타나지만 그래프 상 초반부에서의 변화로 설명할 수 있다. 이 경우, 알고리즘 개선과 최적화 덕분에 비용이 10배 감소하고 성능은 향상되는 결과를 보이고 있다.

분명히 말하자면, DeepSeek은 이 정도의 비용과 성능을 처음으로 달성했다는 점에서 독보적이다. 그들이 공개 가중치(Open Weight)를 공개한 점 또한 독특하지만, 이전의 Mistral과 Llama 모델들도 과거에 이런 사례를 보여주었다. DeepSeek은 이 정도의 비용 수준을 달성했으나, 연말까지 비용이 또 5배 더 하락하더라도 놀라지 말아야 한다.

### 4.3 R1의 성능이 o1과 비슷한 수준인가? ###

그 질문에 대한 답변은, 추론(reasoning)이 기존 패러다임보다 훨씬 빠른 반복 속도와 적은 계산량으로도 의미 있는 성과를 낼 수 있는 새로운 패러다임이라는 점이다. [SemiAnalysis의 scaling laws 보고서에서](https://github.com/synabreu/SemiAnalysisKor/blob/main/20241211-scalinglaw.md) 설명한 바와 같이, 기존 패러다임은 사전 훈련(pre-training)에 의존했는데, 이 방식은 점점 더 비용이 많이 들고 강력한 성과를 내기 어려워지고 있다.

새로운 패러다임은 기존 모델에 대해 합성 데이터 생성(synthetic data generation)과 사후처리 훈련(post-training) 단계에서 강화학습(RL)을 활용하여 추론 능력에 집중함으로써, 낮은 비용으로 빠른 성과를 얻을 수 있게 한다. 낮은 진입 장벽과 쉬운 최적화 덕분에 DeepSeek은 평소보다 빠르게 o1 방식의 기법들을 재현할 수 있었다. 앞으로 이 새로운 패러다임 내에서 다른 업체들이 확장하는 방법을 찾아 감에 따라, 동일한 성능을 달성하기 위한 시간 간격은 더욱 벌어질 것으로 예상된다.

참고로, R1 논문은 사용된 계산 자원에 대해 전혀 언급하지 않는다. 이는 우연이 아니라, R1의 후처리 훈련을 위해 합성 데이터를 생성하는 데 상당한 계산 자원이 필요하기 때문이다. 강화학습에 대해서는 말할 것도 없다. R1은 매우 우수한 모델이며, 추론 성능의 우위를 이처럼 빠르게 따라잡은 것은 객관적으로 인상적이다. 게다가 DeepSeek이 중국 소속임에도 불구하고 적은 자원으로 이를 달성했다는 사실은 더욱 놀라운 점이다.

하지만 R1이 언급한 일부 벤치마크는 오해의 소지가 있다. R1과 o1을 비교하는 것은 까다운데, R1은 자신들이 선두를 달리지 않는 벤치마크는 의도적으로 언급하지 않기 때문이다. 또한, R1이 추론 성능에서는 o1과 맞먹지만 모든 지표에서 명확한 승자가 되지는 않으며, 많은 경우에 o1보다 성능이 떨어지는 부분도 있다.

그리고 아직 o3에 대해서는 언급하지 않았다. o3는 R1이나 o1보다 훨씬 높은 능력을 가지고 있다. 실제로 OpenAI는 최근 o3의 결과를 공유했으며, 벤치마크 스케일링은 수직적으로 진행되고 있다. “딥러닝은 한계에 다다랐다”는 말이 있지만, 이는 또 다른 종류의 한계다.

### 5. R1만큼 좋은 구글 추론 모델 ###

R1에 대한 과도한 관심이 집중되고 있는 가운데, 2.5조 달러 규모의 미국 기업이 한 달 전에 더 저렴한 가격으로 추론 모델을 공개했다: 바로 Google의 Gemini Flash 2.0 Thinking이다. 이 모델은 사용이 가능하며, API를 통한 훨씬 더 긴 컨텍스트 길이를 제공함에도 불구하고 R1보다 상당히 저렴하다.

보고된 벤치마크 결과에 따르면, Flash 2.0 Thinking은 R1을 능가하지만, 벤치마크가 모든 것을 설명하는 것은 아니다. Google은 단 3개의 벤치마크만 공개했기 때문에 전체적인 모습을 파악하기에는 불완전하다. 그럼에도 불구하고, Semi Analysis는 Google의 모델이 견고하다고 평가하며, 여러 면에서 R1에 버금가면서도 과도한 관심을 받지 않고 있다고 본다. 이는 Google의 미흡한 시장 진입 전략과 열악한 사용자 경험 때문일 수도 있지만, 동시에 R1이 중국에서 갑작스럽게 등장한 서프라이즈라는 점도 있다.

분명히 말하자면, 이 모든 것이 DeepSeek의 놀라운 성과를 깎아내리는 것은 아니다. 빠르게 움직이고, 자금이 풍부하며, 똑똑하고 집중력 있는 스타트업 구조 덕분에 DeepSeek은 Meta와 같은 거대 기업들을 능가하며 추론 모델을 출시할 수 있었고, 이는 칭찬할 만하다.

### 6. 기술적 성과 ###

DeepSeek은 기존의 선도 연구소들이 아직 이루지 못한 혁신적인 기술을 해독하고 구현해냈다. SemiAnalysis는 DeepSeek의 모든 공개된 혁신이 서구 연구소들에 의해 거의 즉시 복제될 것이라고 예상한다. 이러한 혁신은 무엇일까? 대부분의 아키텍처적 성과는 V3와 관련이 있으며, V3는 R1의 기본 모델이기도 하다. 이 혁신들에 대해 자세히 알아보자.

### 6.1 사전 및 사후 훈련 ###

DeepSeek V3는 이전에 보지 못한 규모의 MTP(Multi-Token Prediction) 을 활용한다. 이는 단일 토큰 대신 다음 몇 개의 토큰을 예측하는 추가 어텐션 모듈을 도입한 것으로, 훈련 중 모델 성능을 향상시키며 추론 시에는 해당 모듈을 버릴 수 있다. 이는 낮은 계산량으로 성능을 개선할 수 있게 해주는 알고리즘 혁신의 한 예이다. 또한, FP8 정확도를 적용하는 등의 추가 고려사항도 있지만, 미국의 선도 연구소들은 이미 오랫동안 FP8 훈련을 수행해왔다.

그리고 DeepSeek V3는 MoE(Mixture of Experts) 모델로, 이는 하나의 대규모 모델이 여러 분야에 특화된 소규모 전문가들로 구성된 형태로 나타난다. MoE 모델이 직면했던 문제 중 하나는 어떤 토큰이 어떤 하위 모델, 즉 "전문가(expert)" 에게 할당되어야 하는지를 결정하는 것이었다. DeepSeek은 "게이팅 네트워크(Gating Network)" 를 구현하여 토큰을 적절한 전문가에게 균형 있게 분배함으로써 모델 성능에 영향을 주지 않도록 했다. 이 방식은 라우팅 효율을 매우 높이며, 훈련 중 전체 모델 크기에 비해 토큰 당 변경되는 파라미터 수가 극히 적어 훈련 효율성과 추론 비용 절감에 크게 기여한다.

MoE의 효율성 향상이 투자 감소로 이어질 수 있다는 우려에도 불구하고, Dario는 보다 강력한 AI 모델이 가져다주는 경제적 이익이 매우 크기 때문에, 비용 절감 효과가 곧바로 더 큰 모델 구축에 재투자된다고 지적한다. 즉, 전체 투자가 줄어들기보다는 MoE의 효율성 향상이 AI 확장 노력을 가속화할 것이다. 기업들은 모델의 계산 능력을 확장하고 알고리즘 효율성을 높이는 데 집중하고 있다.

R1의 경우, 강력한 기본 모델(v3)을 갖추고 있다는 점에서 큰 혜택을 받았다. 이는 부분적으로 강화학습(RL)의 영향 때문이다. RL에서는 두 가지 측면에 집중되었다: 하나는 포맷팅(일관된 출력을 제공하도록 보장)이고, 다른 하나는 유용성과 무해성을 확보하는 것이다. 추론 능력은 합성 데이터셋을 활용한 모델의 미세 조정 과정에서 나타났다. 이는 SemiAnalysis의 scaling laws 기사에서 o1의 경우와 같이 설명된 바 있다. 참고로, R1 논문에서는 사용된 계산 자원에 대해 언급하지 않는데, 이는 사용된 계산량을 공개할 경우 실제보다 더 많은 GPU를 보유하고 있음을 드러내기 때문이다. 이 정도 규모의 RL은 특히 합성 데이터를 생성하는 데 상당한 계산 자원을 필요로 한다.

추가로, DeepSeek이 사용한 데이터의 일부는 OpenAI의 모델에서 나온 데이터로 보이며, 이는 출력 결과로부터 증류(distillation)하는 것에 관한 정책에 영향을 미칠 것으로 보인다. 이는 이미 서비스 약관상 불법이지만, 앞으로 증류를 막기 위해 고객 확인(KYC, Know Your Customer)의 한 형태가 새로운 추세가 될 수 있다.

그리고 증류(distillation)에 관해서 말하자면, R1 논문의 가장 흥미로운 부분은 추론 능력이 없는 소형 모델들을 추론 모델의 출력 결과로 미세 조정함으로써 추론 모델로 전환할 수 있다는 점이다. 데이터셋 큐레이션에는 총 80만 개의 샘플이 포함되었으며, 이제 누구나 R1의 CoT(Chain-of-Thought) 출력 결과를 활용해 자신만의 데이터셋을 만들고, 그 결과를 바탕으로 추론 모델을 구축할 수 있게 되었다. Semi Analysis는 앞으로 더 많은 소형 모델들이 추론 능력을 갖추게 되어 소형 모델의 성능을 강화하는 모습을 보게 될 것으로 예상한다.

### 6.2 사전 및 사후 훈련 ###

MLA는 DeepSeek의 추론 비용을 크게 줄이는 핵심 혁신 요소이다. 그 이유는 MLA가 표준 어텐션에 비해 쿼리당 필요한 KV 캐시의 양을 약 93.3% 줄여주기 때문이다. KV 캐시는 트랜스포머 모델에서 대화의 문맥을 나타내는 데이터를 저장하는 메커니즘이며, 불필요한 연산을 줄인다.

Semi Analysis의 스케일링 법칙(article)에서 논의한 바와 같이, 대화의 문맥이 길어짐에 따라 KV 캐시의 크기도 커져 상당한 메모리 제약이 발생한다. 쿼리당 필요한 KV 캐시의 양을 대폭 줄이면 쿼리당 필요한 하드웨어 양도 줄어들어 비용이 감소한다. 그러나 DeepSeek는 시장 점유율 확보를 위해 비용을 낮추어 추론을 제공하고 있으며, 실제로 수익을 내지 않고 있다. Google Gemini Flash 2는 여전히 더 저렴한 추론을 제공하며, Google이 이를 비용 부담 없이 제공할 가능성은 낮다.

MLA는 특히 미국의 많은 선도 연구소들의 관심을 끌었다. MLA는 2024년 5월에 출시된 DeepSeek V2에서 도입되었다. DeepSeek는 또한 H100에 비해 메모리 대역폭과 용량이 높은 H20를 통해 추론 워크로드에 있어 더 큰 효율성을 누리고 있다. 또한 Huawei와의 파트너십을 발표하였으나, 현재까지 Ascend 계산(Compute)과와 거의 진행된 바가 없다.

Semi Analysis는 가장 흥미로운 영향이 특히 이익률에 미치는 효과와, 그것이 전체 생태계에 의미하는 바에 있다고 믿는다. 아래에서는 전체 AI 산업의 미래 가격 구조에 대한 전망을 제시하고, DeepSeek가 가격을 보조하고 있다고 생각하는 이유와 동시에 Jevons 역설 의 초기 징후가 나타나고 있다고 보는 이유를 상세히 설명한다. 또한 수출 통제에 미치는 영향, DeepSeek의 지배력이 강화될 경우 중국공산당(CCP)이 어떻게 반응할지 등에 대해 논평한다.


